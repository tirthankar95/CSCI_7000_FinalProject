{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXRx6W6d7U6lO66RuBEdbe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALAIPri6MIxa","executionInfo":{"status":"ok","timestamp":1670091291575,"user_tz":420,"elapsed":2525,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}},"outputId":"1dbf14e7-8865-4268-b08a-dfcac1c25baf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'CSCI_7000_FinalProject'...\n","remote: Enumerating objects: 363, done.\u001b[K\n","remote: Counting objects: 100% (363/363), done.\u001b[K\n","remote: Compressing objects: 100% (273/273), done.\u001b[K\n","remote: Total 363 (delta 129), reused 315 (delta 81), pack-reused 0\u001b[K\n","Receiving objects: 100% (363/363), 19.57 MiB | 16.91 MiB/s, done.\n","Resolving deltas: 100% (129/129), done.\n"]}],"source":["!git clone https://github.com/tirthankar95/CSCI_7000_FinalProject.git"]},{"cell_type":"code","source":["%cd CSCI_7000_FinalProject/Ajay/gym-minigrid/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HD0W_MbuMq2S","executionInfo":{"status":"ok","timestamp":1670091310223,"user_tz":420,"elapsed":4,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}},"outputId":"014538fb-bd8e-44b2-ab14-e07f8839c98a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/CSCI_7000_FinalProject/Ajay/gym-minigrid\n"]}]},{"cell_type":"code","source":["!pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6_AVuwNaMwEU","executionInfo":{"status":"ok","timestamp":1670091336048,"user_tz":420,"elapsed":12378,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}},"outputId":"7e8f67de-a826-4789-f9e6-a9ff9c4e1461"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Obtaining file:///content/CSCI_7000_FinalProject/Ajay/gym-minigrid\n","Requirement already satisfied: gym>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from gym-minigrid==0.0.5) (0.25.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from gym-minigrid==0.0.5) (1.21.6)\n","Collecting pyqt5>=5.10.1\n","  Downloading PyQt5-5.15.7-cp37-abi3-manylinux1_x86_64.whl (8.4 MB)\n","\u001b[K     |████████████████████████████████| 8.4 MB 38.3 MB/s \n","\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.9.6->gym-minigrid==0.0.5) (1.5.0)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.9.6->gym-minigrid==0.0.5) (4.13.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.9.6->gym-minigrid==0.0.5) (0.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.9.6->gym-minigrid==0.0.5) (3.10.0)\n","Collecting PyQt5-sip<13,>=12.11\n","  Downloading PyQt5_sip-12.11.0-cp38-cp38-manylinux1_x86_64.whl (361 kB)\n","\u001b[K     |████████████████████████████████| 361 kB 69.0 MB/s \n","\u001b[?25hCollecting PyQt5-Qt5>=5.15.0\n","  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n","\u001b[K     |████████████████████████████████| 59.9 MB 1.2 MB/s \n","\u001b[?25hInstalling collected packages: PyQt5-sip, PyQt5-Qt5, pyqt5, gym-minigrid\n","  Running setup.py develop for gym-minigrid\n","Successfully installed PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.0 gym-minigrid-0.0.5 pyqt5-5.15.7\n"]}]},{"cell_type":"code","source":["from gym_minigrid.envs.doorkey import *\n","env = DoorKeyEnv(size=6)\n","env.step_m(1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QHRf-tShMzMM","executionInfo":{"status":"ok","timestamp":1670091350913,"user_tz":420,"elapsed":855,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}},"outputId":"4ccb1f81-cf12-46c7-858e-440981970be9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/content/CSCI_7000_FinalProject/Ajay/gym-minigrid/gym_minigrid/roomgrid.py:300: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n","  if front_cell is None or front_cell.type is 'wall':\n","/content/CSCI_7000_FinalProject/Ajay/gym-minigrid/gym_minigrid/minigrid.py:663: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  mask = np.zeros(shape=(grid.width, grid.height), dtype=np.bool)\n"]},{"output_type":"execute_result","data":{"text/plain":["({'image': array([[[0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0]],\n","  \n","         [[0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0]],\n","  \n","         [[0, 0, 0],\n","          [0, 0, 0],\n","          [2, 5, 0],\n","          [2, 5, 0],\n","          [2, 5, 0],\n","          [4, 4, 2],\n","          [2, 5, 0]],\n","  \n","         [[0, 0, 0],\n","          [0, 0, 0],\n","          [2, 5, 0],\n","          [5, 4, 0],\n","          [1, 0, 0],\n","          [1, 0, 0],\n","          [1, 0, 0]],\n","  \n","         [[0, 0, 0],\n","          [0, 0, 0],\n","          [2, 5, 0],\n","          [2, 5, 0],\n","          [2, 5, 0],\n","          [2, 5, 0],\n","          [2, 5, 0]],\n","  \n","         [[0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0]],\n","  \n","         [[0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0],\n","          [0, 0, 0]]], dtype=uint8),\n","  'direction': 1,\n","  'mission': 'use the key to open the door and then get to the goal'},\n"," -1,\n"," False,\n"," (1, 1))"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","import random\n","import gym_minigrid\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import keras\n","from keras.models import Sequential,Model\n","from keras.layers import LSTM,Bidirectional,Dense,Input,Embedding,TimeDistributed\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n"],"metadata":{"id":"Ufnk-Hx0M5wh","executionInfo":{"status":"ok","timestamp":1670091372018,"user_tz":420,"elapsed":4329,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["num_actions=5"],"metadata":{"id":"d12pNDSOM-K-","executionInfo":{"status":"ok","timestamp":1670091375711,"user_tz":420,"elapsed":292,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["\n","def plotProgress(reward_plot):\n","    plt.plot(reward_plot)\n","    plt.xlabel('Episodes')\n","    plt.ylabel('Avg. reward')\n","    plt.title('Avg Reward Per Step V/S Episodes.')\n","    plt.show()\n"],"metadata":{"id":"2yozA0MIM__e","executionInfo":{"status":"ok","timestamp":1670091382983,"user_tz":420,"elapsed":292,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["def create():\n","    global num_actions\n","    input=Input(shape=(3,147)) # (3,3,7,7) ~ the mini-grid by default returns (3,7,7) image.\n","    model=LSTM(units=32,return_sequences=False)(input)\n","    output=Dense(units=num_actions,activation='relu')(model)\n","    model=Model(input,output)\n","    return model\n"],"metadata":{"id":"A4xDRnJ-NB1i","executionInfo":{"status":"ok","timestamp":1670091389401,"user_tz":420,"elapsed":297,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","global num_actions\n","# Configuration paramaters for the whole setup\n","seed = 42\n","gamma = 0.99  # Discount factor for past rewards\n","epsilon = 1.0  # Epsilon greedy parameter\n","epsilon_min = 0.1  # Minimum epsilon greedy parameter\n","epsilon_max = 1.0  # Maximum epsilon greedy parameter\n","epsilon_interval = (\n","    epsilon_max - epsilon_min\n",")  # Rate at which to reduce chance of random action being taken\n","batch_size = 4098  # Size of batch taken from replay buffer\n","max_steps_per_episode = 5000 #beast 1000\n","from gym_minigrid.envs.doorkey import *\n","env = DoorKeyEnv(size=6)\n","#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n","env.seed(seed)\n","\n","model=create()\n","model_target=create()\n","loss_function = keras.losses.MeanSquaredError()\n","optimizer=keras.optimizers.RMSprop()\n","\n","# Experience replay buffers\n","action_history = []\n","state_history = []\n","state_next_history = []\n","rewards_history = []\n","episode_reward_history=[]\n","done_history = []\n","reward_plot = []\n","running_reward = 0\n","episode_count = 0\n","frame_count = 0\n","# Number of frames to take random action and observe output\n","epsilon_random_frames = 1000 #beast 10000\n","# Number of frames for exploration\n","epsilon_greedy_frames = 1000000.0\n","# Maximum replay length\n","# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n","max_memory_length = 100000\n","# Train the model after 4 actions\n","update_after_actions = 100\n","# How often to update the target network\n","update_target_network = 10000\n","# Using huber loss for stability\n","# We are taking 3 frames in our LSTM\n","frame_offset=2\n","\n"],"metadata":{"id":"LW0DguegNDVH","executionInfo":{"status":"ok","timestamp":1670091438259,"user_tz":420,"elapsed":1009,"user":{"displayName":"AJAY NARASIMHA Mopidevi","userId":"18384819973190404801"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["noOfEpisodes=1000 #beast 100000\n","#while noOfEpisodes:  # Run until solved\n","for _ in tqdm(range(noOfEpisodes)):\n","    #noOfEpisodes-=1\n","    state = np.array(env.reset_m())\n","    episode_reward = 0\n","\n","    for timestep in range(1, max_steps_per_episode):\n","        # env.render(); Adding this line would show the attempts\n","        # of the agent in a pop up window.\n","        frame_count += 1\n","\n","        # Use epsilon-greedy for exploration\n","        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n","            # Take random action\n","            action = np.random.choice(num_actions)\n","        else:\n","            # Predict action Q-values\n","            # From environment state\n","            state_numpy = np.array(state_history[-3:]).reshape(3,147)\n","            state_numpy = np.array([state_numpy])\n","            action_probs = model(state_numpy, training=False)\n","            # Take best action\n","            action = np.argmax(action_probs[0])\n","\n","        # Decay probability of taking random action\n","        epsilon -= epsilon_interval / epsilon_greedy_frames\n","        epsilon = max(epsilon, epsilon_min)\n","\n","        # Apply the sampled action in our environment\n","        state_next, reward, done, _ = env.step_m(action)\n","        state_next = np.array(state_next)\n","\n","        episode_reward += reward\n","\n","        # Save actions and states in replay buffer\n","        action_history.append(action)\n","        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n","        state_history.append(temp_state['image'])\n","        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n","        state_next_history.append(temp_state['image'])\n","        done_history.append(done)\n","        rewards_history.append(reward)\n","        state = state_next\n","\n","        # Update every fourth frame and once batch size is over 32\n","        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n","            # Get indices of samples for replay buffers\n","            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n","            i=indices[0]\n","            # Using list comprehension to sample from replay buffer\n","            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape(3,147) for i in indices])\n","            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape(3,147) for i in indices])\n","            rewards_sample = [rewards_history[i] for i in indices]\n","            action_sample = [action_history[i] for i in indices]\n","            # Build the updated Q-values for the sampled future states\n","            # Use the target model for stability\n","            future_rewards = model_target.predict(state_next_sample,verbose=False)\n","            # Q value = reward + discount factor * expected future reward\n","            updated_q_values = rewards_sample + gamma * np.max(\\\n","                future_rewards, axis=1)\n","            updated_q_values = updated_q_values.astype('float32')\n","            # Create a mask so we only calculate loss on the updated Q-values\n","            masks = tf.one_hot(action_sample, num_actions)\n","            with tf.GradientTape() as tape:    \n","                q_values = model(state_sample)\n","                # Apply the masks to the Q-values to get the Q-value for action taken\n","                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n","                loss = loss_function(updated_q_values,q_action)\n","            # Backpropagation\n","            grads = tape.gradient(loss, model.trainable_variables)\n","            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n","            \n","\n","        if frame_count % update_target_network == 0:\n","            # update the the target network with new weights\n","            model_target.set_weights(model.get_weights())\n","            # Log details\n","            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n","            print(template.format(running_reward, episode_count, frame_count))\n","\n","        # Limit the state and reward history\n","        if len(rewards_history) > max_memory_length:\n","            del rewards_history[:1]\n","            del state_history[:1]\n","            del state_next_history[:1]\n","            del action_history[:1]\n","            del done_history[:1]\n","        if done:\n","            break\n","\n","# Update running reward to check condition for solving\n","    episode_reward_history.append(episode_reward)\n","    running_reward = np.mean(episode_reward_history)\n","    reward_plot.append(episode_reward/timestep)\n","    episode_count += 1\n","plotProgress(reward_plot)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-j58-TWNHBt","outputId":"ebeed344-6f20-4fd6-8978-5a6b61adbc30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["  1%|          | 10/1000 [00:21<50:21,  3.05s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1964.20 at episode 10, frame count 10000\n"]},{"output_type":"stream","name":"stderr","text":["  2%|▏         | 20/1000 [01:08<47:35,  2.91s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2455.35 at episode 20, frame count 20000\n"]},{"output_type":"stream","name":"stderr","text":["  3%|▎         | 33/1000 [01:46<47:46,  2.96s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2196.27 at episode 33, frame count 30000\n"]},{"output_type":"stream","name":"stderr","text":["  4%|▍         | 41/1000 [02:28<1:33:45,  5.87s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2401.37 at episode 41, frame count 40000\n"]},{"output_type":"stream","name":"stderr","text":["  5%|▍         | 49/1000 [03:13<1:09:45,  4.40s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2543.42 at episode 48, frame count 50000\n"]},{"output_type":"stream","name":"stderr","text":["  6%|▋         | 63/1000 [03:54<56:43,  3.63s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2384.65 at episode 63, frame count 60000\n"]},{"output_type":"stream","name":"stderr","text":["  8%|▊         | 78/1000 [04:38<40:45,  2.65s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2233.32 at episode 78, frame count 70000\n"]},{"output_type":"stream","name":"stderr","text":["  9%|▉         | 89/1000 [05:18<54:11,  3.57s/it]  "]},{"output_type":"stream","name":"stdout","text":["running reward: -2217.19 at episode 89, frame count 80000\n"]},{"output_type":"stream","name":"stderr","text":[" 11%|█         | 106/1000 [05:59<20:58,  1.41s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2073.55 at episode 106, frame count 90000\n"]},{"output_type":"stream","name":"stderr","text":[" 12%|█▏        | 120/1000 [06:47<51:44,  3.53s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2054.68 at episode 120, frame count 100000\n"]},{"output_type":"stream","name":"stderr","text":[" 13%|█▎        | 132/1000 [07:31<57:06,  3.95s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2034.72 at episode 132, frame count 110000\n"]},{"output_type":"stream","name":"stderr","text":[" 14%|█▍        | 140/1000 [08:16<1:16:14,  5.32s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2083.84 at episode 140, frame count 120000\n"]},{"output_type":"stream","name":"stderr","text":[" 15%|█▌        | 153/1000 [09:06<38:08,  2.70s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2069.81 at episode 153, frame count 130000\n"]},{"output_type":"stream","name":"stderr","text":[" 17%|█▋        | 169/1000 [09:56<40:29,  2.92s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -2016.17 at episode 169, frame count 140000\n"]},{"output_type":"stream","name":"stderr","text":[" 19%|█▊        | 186/1000 [10:46<38:00,  2.80s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1954.85 at episode 186, frame count 150000\n"]},{"output_type":"stream","name":"stderr","text":[" 20%|██        | 201/1000 [11:27<40:04,  3.01s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1904.03 at episode 201, frame count 160000\n"]},{"output_type":"stream","name":"stderr","text":[" 22%|██▏       | 215/1000 [12:23<34:51,  2.66s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1899.88 at episode 215, frame count 170000\n"]},{"output_type":"stream","name":"stderr","text":[" 23%|██▎       | 228/1000 [13:17<56:58,  4.43s/it]  "]},{"output_type":"stream","name":"stdout","text":["running reward: -1900.40 at episode 228, frame count 180000\n"]},{"output_type":"stream","name":"stderr","text":[" 24%|██▍       | 242/1000 [14:07<43:51,  3.47s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1883.01 at episode 242, frame count 190000\n"]},{"output_type":"stream","name":"stderr","text":[" 26%|██▌       | 256/1000 [14:57<35:32,  2.87s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1867.63 at episode 256, frame count 200000\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 272/1000 [15:53<38:47,  3.20s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1846.37 at episode 271, frame count 210000\n"]},{"output_type":"stream","name":"stderr","text":[" 29%|██▊       | 286/1000 [16:42<40:48,  3.43s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1828.72 at episode 286, frame count 220000\n"]},{"output_type":"stream","name":"stderr","text":[" 30%|███       | 300/1000 [17:32<51:37,  4.43s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1812.47 at episode 300, frame count 230000\n"]},{"output_type":"stream","name":"stderr","text":[" 31%|███       | 311/1000 [18:25<44:34,  3.88s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1820.94 at episode 311, frame count 240000\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 326/1000 [19:18<40:57,  3.65s/it]"]},{"output_type":"stream","name":"stdout","text":["running reward: -1803.55 at episode 326, frame count 250000\n"]},{"output_type":"stream","name":"stderr","text":[" 33%|███▎      | 332/1000 [19:51<49:00,  4.40s/it]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ln6JyiT0NPcU"},"execution_count":null,"outputs":[]}]}