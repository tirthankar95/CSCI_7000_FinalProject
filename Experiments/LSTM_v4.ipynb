{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///C:/Users/Ajay%20Narasimha/Documents/DeepRL/Project/Experts/CSCI_7000_FinalProject-master/gym-minigrid\n",
      "Requirement already satisfied: gym>=0.25.2 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from gym-minigrid==0.0.5) (0.25.2)\n",
      "Requirement already satisfied: numpy>=1.21.6 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from gym-minigrid==0.0.5) (1.21.6)\n",
      "Requirement already satisfied: pyqt5>=5.10.1 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from gym-minigrid==0.0.5) (5.15.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0; python_version < \"3.10\" in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from gym>=0.25.2->gym-minigrid==0.0.5) (5.0.0)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from gym>=0.25.2->gym-minigrid==0.0.5) (1.5.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from gym>=0.25.2->gym-minigrid==0.0.5) (0.0.8)\n",
      "Requirement already satisfied: PyQt5-Qt5>=5.15.0 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from pyqt5>=5.10.1->gym-minigrid==0.0.5) (5.15.2)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.11 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from pyqt5>=5.10.1->gym-minigrid==0.0.5) (12.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.8.0; python_version < \"3.10\"->gym>=0.25.2->gym-minigrid==0.0.5) (3.1.0)\n",
      "Installing collected packages: gym-minigrid\n",
      "  Attempting uninstall: gym-minigrid\n",
      "    Found existing installation: gym-minigrid 0.0.5\n",
      "    Uninstalling gym-minigrid-0.0.5:\n",
      "      Successfully uninstalled gym-minigrid-0.0.5\n",
      "  Running setup.py develop for gym-minigrid\n",
      "Successfully installed gym-minigrid\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay Narasimha\\Documents\\DeepRL\\Project\\Experts\\CSCI_7000_FinalProject-master\\gym-minigrid\\gym_minigrid\\roomgrid.py:300: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if front_cell is None or front_cell.type is 'wall':\n",
      "C:\\Users\\Ajay Narasimha\\anaconda3\\lib\\site-packages\\gym\\utils\\seeding.py:63: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\Ajay Narasimha\\Documents\\DeepRL\\Project\\Experts\\CSCI_7000_FinalProject-master\\gym-minigrid\\gym_minigrid\\minigrid.py:663: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask = np.zeros(shape=(grid.width, grid.height), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'image': array([[[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [4, 4, 2]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [5, 4, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]]], dtype=uint8),\n",
       "  'direction': 1,\n",
       "  'mission': 'avoid the lava and get to the green goal square'},\n",
       " -1,\n",
       " False,\n",
       " (1, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "#env = CrossingEnv(size=6)\n",
    "env = MixedEnv(size=8)\n",
    "env.step_m(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (3.2.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: numpy>=1.11 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from matplotlib) (1.21.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (4.64.0)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\ajay narasimha\\anaconda3\\lib\\site-packages (from tqdm) (0.4.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras\n",
    "!pip install matplotlib\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay Narasimha\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\Ajay Narasimha\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM,Bidirectional,Dense,Input,Embedding,TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plotProgress(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg. reward')\n",
    "    plt.title('Avg Reward Per Step V/S Episodes.')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_offset = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create():\n",
    "    global num_actions\n",
    "    input=Input(shape=(8,147)) # (3,3,7,7) ~ the mini-grid by default returns (3,7,7) image.\n",
    "    model=LSTM(units=256,return_sequences=False)(input)\n",
    "    x1 = Dense(units=256, activation='relu')(model)\n",
    "    x1 = Dense(units=128, activation='relu')(x1)\n",
    "    x1 = Dense(units=64, activation='relu')(x1)\n",
    "    x1 = Dense(units=32, activation='relu')(x1)\n",
    "    x1 = Dense(units=16, activation='relu')(x1)\n",
    "    output=Dense(units=num_actions,activation='linear')(x1)\n",
    "    model=Model(input,output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x174f4597370>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ajay Narasimha\\anaconda3\\lib\\site-packages\\gym\\utils\\seeding.py:63: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
      "  deprecation(\n",
      "C:\\Users\\Ajay Narasimha\\Documents\\DeepRL\\Project\\Experts\\CSCI_7000_FinalProject-master\\gym-minigrid\\gym_minigrid\\minigrid.py:663: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask = np.zeros(shape=(grid.width, grid.height), dtype=np.bool)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "global num_actions\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.9  # Discount factor for past rewards\n",
    "epsilon = 0.8  # Epsilon greedy parameter\n",
    "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 4096  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 5000 #beast 1000\n",
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "env = CrossingEnv(size=8)\n",
    "#env = CrossingEnv(size=6)\n",
    "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
    "#env.seed(seed)\n",
    "\n",
    "model=create()\n",
    "model_target=create()\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer=keras.optimizers.RMSprop()\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "episode_reward_history=[]\n",
    "done_history = []\n",
    "reward_plot = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 100000 #beast 10000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 500000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 1000000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 100\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "# We are taking 3 frames in our LSTM\n",
    "#frame_offset=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "termination_steps=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_ratio = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(42)\n",
    "def best_action(action_probs):\n",
    "    action = np.argmax(action_probs)\n",
    "    action_probs = np.abs(action_probs - action_probs[action])\n",
    "    possible_actions = []\n",
    "    for i in range(5):\n",
    "        if(action_probs[i]<0.1):\n",
    "            possible_actions.append(i)\n",
    "    #print(action_probs, possible_actions)\n",
    "    length = len(possible_actions)\n",
    "    action_probs = [1/length for i in possible_actions]\n",
    "    return np.random.choice(possible_actions, p = action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|█████████████████████████████▉                                                   | 74/200 [01:58<04:53,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: -449.58 at episode 73, frame count 10000 with epsilon 0.7859999999995975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|█████████████████████████████████████▋                                           | 93/200 [02:26<02:41,  1.51s/it]"
     ]
    }
   ],
   "source": [
    "noOfEpisodes=200 #beast 100000\n",
    "#while noOfEpisodes:  # Run until solved\n",
    "for _ in tqdm(range(noOfEpisodes)):\n",
    "    #noOfEpisodes-=1\n",
    "    state = np.array(env.reset_m())\n",
    "    episode_reward = 0\n",
    "    success = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):]).reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = model(state_numpy, training=False)\n",
    "            # Take best action\n",
    "            #action = np.argmax(action_probs[0])\n",
    "            action = best_action(action_probs[0])\n",
    "            #print(action_probs[0])\n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step_m(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "        if(done==True):\n",
    "            success = 1\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history.append(temp_state['image'])\n",
    "        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n",
    "        state_next_history.append(temp_state['image'])\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n",
    "            i=indices[0]\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = model_target.predict(state_next_sample,verbose=False)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * np.max(\\\n",
    "                future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values.astype('float32')\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            with tf.GradientTape() as tape:    \n",
    "                q_values = model(state_sample)\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values,q_action)\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            \n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {} with epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, epsilon))\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    success_ratio.append(success)\n",
    "    # Update running reward to check condition for solving\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    reward_plot.append(episode_reward/timestep)\n",
    "    episode_count += 1\n",
    "    termination_steps.append(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotProgress(reward_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm reward_plot.txt\n",
    "!rm termination_steps.txt\n",
    "!rm succes_ratio.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array(reward_plot)\n",
    "np.savetxt(\"reward_plot.txt\",a)\n",
    "b = np.array(termination_steps)\n",
    "np.savetxt('termination_steps.txt',b)\n",
    "c = np.array(success_ratio)\n",
    "np.savetxt(\"success.txt\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotProgressTimesteps(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Total Steps')\n",
    "    plt.title('Total Steps V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressTimesteps(termination_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotProgressSuccess(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Success\\Failure')\n",
    "    plt.title(' Success V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressSuccess(success_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('CrossingEnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
