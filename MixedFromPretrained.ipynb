{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:43:28.709768Z",
     "iopub.status.busy": "2022-12-12T00:43:28.709367Z",
     "iopub.status.idle": "2022-12-12T00:43:34.997492Z",
     "shell.execute_reply": "2022-12-12T00:43:34.996431Z",
     "shell.execute_reply.started": "2022-12-12T00:43:28.709700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:07.005517Z",
     "iopub.status.busy": "2022-12-12T00:44:07.005072Z",
     "iopub.status.idle": "2022-12-12T00:44:09.054114Z",
     "shell.execute_reply": "2022-12-12T00:44:09.053039Z",
     "shell.execute_reply.started": "2022-12-12T00:44:07.005486Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/tirthankar95/CSCI_7000_FinalProject.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:43:59.503786Z",
     "iopub.status.busy": "2022-12-12T00:43:59.503374Z",
     "iopub.status.idle": "2022-12-12T00:44:00.313224Z",
     "shell.execute_reply": "2022-12-12T00:44:00.312281Z",
     "shell.execute_reply.started": "2022-12-12T00:43:59.503757Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:15.608270Z",
     "iopub.status.busy": "2022-12-12T00:44:15.607844Z",
     "iopub.status.idle": "2022-12-12T00:44:15.614492Z",
     "shell.execute_reply": "2022-12-12T00:44:15.613536Z",
     "shell.execute_reply.started": "2022-12-12T00:44:15.608240Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd CSCI_7000_FinalProject/gym-minigrid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:26.020037Z",
     "iopub.status.busy": "2022-12-12T00:44:26.019658Z",
     "iopub.status.idle": "2022-12-12T00:44:26.023649Z",
     "shell.execute_reply": "2022-12-12T00:44:26.022861Z",
     "shell.execute_reply.started": "2022-12-12T00:44:26.020009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T19:11:25.179249Z",
     "iopub.status.busy": "2022-12-10T19:11:25.178863Z",
     "iopub.status.idle": "2022-12-10T19:11:25.993851Z",
     "shell.execute_reply": "2022-12-10T19:11:25.992811Z",
     "shell.execute_reply.started": "2022-12-10T19:11:25.179225Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:32.312962Z",
     "iopub.status.busy": "2022-12-12T00:44:32.312578Z",
     "iopub.status.idle": "2022-12-12T00:44:35.457553Z",
     "shell.execute_reply": "2022-12-12T00:44:35.456782Z",
     "shell.execute_reply.started": "2022-12-12T00:44:32.312937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:42.461790Z",
     "iopub.status.busy": "2022-12-12T00:44:42.461270Z",
     "iopub.status.idle": "2022-12-12T00:44:45.997890Z",
     "shell.execute_reply": "2022-12-12T00:44:45.997122Z",
     "shell.execute_reply.started": "2022-12-12T00:44:42.461747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:50.372049Z",
     "iopub.status.busy": "2022-12-12T00:44:50.371626Z",
     "iopub.status.idle": "2022-12-12T00:44:50.466515Z",
     "shell.execute_reply": "2022-12-12T00:44:50.465750Z",
     "shell.execute_reply.started": "2022-12-12T00:44:50.372020Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "#env = CrossingEnv(size=6)\n",
    "env = MixedEnv(size=8)\n",
    "env.step_m(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:57.838979Z",
     "iopub.status.busy": "2022-12-12T00:44:57.838592Z",
     "iopub.status.idle": "2022-12-12T00:44:58.909986Z",
     "shell.execute_reply": "2022-12-12T00:44:58.909379Z",
     "shell.execute_reply.started": "2022-12-12T00:44:57.838954Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM,Bidirectional,Dense,Input,Embedding,TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:58.912260Z",
     "iopub.status.busy": "2022-12-12T00:44:58.911342Z",
     "iopub.status.idle": "2022-12-12T00:44:58.915783Z",
     "shell.execute_reply": "2022-12-12T00:44:58.914988Z",
     "shell.execute_reply.started": "2022-12-12T00:44:58.912224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_actions=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:00.948553Z",
     "iopub.status.busy": "2022-12-12T00:45:00.948155Z",
     "iopub.status.idle": "2022-12-12T00:45:00.952492Z",
     "shell.execute_reply": "2022-12-12T00:45:00.951743Z",
     "shell.execute_reply.started": "2022-12-12T00:45:00.948528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plotProgress(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg. reward')\n",
    "    plt.title('Avg Reward Per Step V/S Episodes.')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:05.450799Z",
     "iopub.status.busy": "2022-12-12T00:45:05.450402Z",
     "iopub.status.idle": "2022-12-12T00:45:05.454130Z",
     "shell.execute_reply": "2022-12-12T00:45:05.453479Z",
     "shell.execute_reply.started": "2022-12-12T00:45:05.450770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_offset = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:27.174676Z",
     "iopub.status.busy": "2022-12-12T00:45:27.174280Z",
     "iopub.status.idle": "2022-12-12T00:45:27.179842Z",
     "shell.execute_reply": "2022-12-12T00:45:27.179152Z",
     "shell.execute_reply.started": "2022-12-12T00:45:27.174651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create():\n",
    "    global num_actions\n",
    "    input=Input(shape=((frame_offset+1),147)) # (3,3,7,7) ~ the mini-grid by default returns (3,7,7) image.\n",
    "    model=LSTM(units=256,return_sequences=False)(input)\n",
    "    x1 = Dense(units=256, activation='relu')(model)\n",
    "    x1 = Dense(units=128, activation='relu')(x1)\n",
    "    x1 = Dense(units=64, activation='relu')(x1)\n",
    "    x1 = Dense(units=32, activation='relu')(x1)\n",
    "    x1 = Dense(units=16, activation='relu')(x1)\n",
    "    output=Dense(units=num_actions,activation='linear')(x1)\n",
    "    model=Model(input,output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:28.935891Z",
     "iopub.status.busy": "2022-12-12T00:45:28.935509Z",
     "iopub.status.idle": "2022-12-12T00:45:30.140921Z",
     "shell.execute_reply": "2022-12-12T00:45:30.140151Z",
     "shell.execute_reply.started": "2022-12-12T00:45:28.935865Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:32:30.119686Z",
     "iopub.status.busy": "2022-12-10T21:32:30.119278Z",
     "iopub.status.idle": "2022-12-10T21:32:30.764116Z",
     "shell.execute_reply": "2022-12-10T21:32:30.763325Z",
     "shell.execute_reply.started": "2022-12-10T21:32:30.119660Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "global num_actions\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 0.8  # Epsilon greedy parameter\n",
    "epsilon_min = 0.2  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 4096  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 5000 #beast 1000\n",
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "# env = DoorKeyEnv(size=8)\n",
    "# env = CrossingEnv(size=8)\n",
    "env = MixedEnv(size=8)\n",
    "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
    "#env.seed(seed)\n",
    "\n",
    "mixed_model=create()\n",
    "model_target=create()\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer=keras.optimizers.RMSprop()\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "episode_reward_history=[]\n",
    "done_history = []\n",
    "reward_plot = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 100000 #beast 10000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 500000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 500000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 100\n",
    "# How often to update the target network\n",
    "update_target_network = 100000\n",
    "# Using huber loss for stability\n",
    "# We are taking 3 frames in our LSTM\n",
    "#frame_offset=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:32:33.494243Z",
     "iopub.status.busy": "2022-12-10T21:32:33.493837Z",
     "iopub.status.idle": "2022-12-10T21:32:33.498163Z",
     "shell.execute_reply": "2022-12-10T21:32:33.497249Z",
     "shell.execute_reply.started": "2022-12-10T21:32:33.494217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_factor = 0.9\n",
    "model_probs = [0.333, 0.333, 0.334]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:34:09.767584Z",
     "iopub.status.busy": "2022-12-10T21:34:09.767183Z",
     "iopub.status.idle": "2022-12-10T21:34:14.004703Z",
     "shell.execute_reply": "2022-12-10T21:34:14.003954Z",
     "shell.execute_reply.started": "2022-12-10T21:34:09.767559Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed_agent_targets = []\n",
    "mixed_agent_targets.append(keras.models.load_model(\"DoorKey_Experts1\"))\n",
    "mixed_agent_targets.append(keras.models.load_model(\"Lava_Experts2\"))\n",
    "mixed_agent_targets.append(create())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:34:14.006304Z",
     "iopub.status.busy": "2022-12-10T21:34:14.005985Z",
     "iopub.status.idle": "2022-12-10T21:34:14.010540Z",
     "shell.execute_reply": "2022-12-10T21:34:14.009582Z",
     "shell.execute_reply.started": "2022-12-10T21:34:14.006280Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_ratio = []\n",
    "termination_steps=[]\n",
    "epsilon_history = [epsilon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 42\n",
    "np.random.seed(seed=42)\n",
    "def best_action(action_probs):\n",
    "  action = np.argmax(action_probs)\n",
    "  action_probs = np.abs(action_probs - action_probs[action])\n",
    "  possible_actions = []\n",
    "  for i in range(5):\n",
    "    if (action_probs[i] < 0.2):\n",
    "      possible_actions.append(i)\n",
    "  # print(action_probs, possible_actions)\n",
    "  length = len(possible_actions)\n",
    "  action_probs = [1 / length for i in possible_actions]\n",
    "  return np.random.choice(possible_actions, p=action_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T22:43:16.476590Z",
     "iopub.status.busy": "2022-12-10T22:43:16.475812Z",
     "iopub.status.idle": "2022-12-10T23:05:21.486566Z",
     "shell.execute_reply": "2022-12-10T23:05:21.485834Z",
     "shell.execute_reply.started": "2022-12-10T22:43:16.476555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noOfEpisodes=100 #beast 100000\n",
    "#while noOfEpisodes:  # Run until solved\n",
    "for _ in tqdm(range(noOfEpisodes)):\n",
    "    #noOfEpisodes-=1\n",
    "    state = np.array(env.reset_m())\n",
    "    episode_reward = 0\n",
    "    success = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            #print(len(state_history))\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):]).reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = mixed_model(state_numpy, training=False)\n",
    "            # Take best action\n",
    "            #action = np.argmax(action_probs[0])\n",
    "            action = best_action(action_probs[0])\n",
    "            #print(action_probs[0])\n",
    "            \n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        #epsilon_history.append(epsilon)\n",
    "        \n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step_m(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "        if(done==True):\n",
    "            success = 1\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history.append(temp_state['image'])\n",
    "        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n",
    "        state_next_history.append(temp_state['image'])\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n",
    "            i=indices[0]\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            #Pick one of the expert or self\n",
    "            target_id = np.random.choice(3,p=model_probs)\n",
    "            model_target = mixed_agent_targets[target_id]\n",
    "            \n",
    "            future_rewards = model_target.predict(state_next_sample,verbose=False)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * np.max(\\\n",
    "                future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values.astype('float32')\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            with tf.GradientTape() as tape:    \n",
    "                q_values = mixed_model(state_sample)\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values,q_action)\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, mixed_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, mixed_model.trainable_variables))\n",
    "            \n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target = mixed_agent_targets[2]\n",
    "            model_target.set_weights(mixed_model.get_weights())\n",
    "            mixed_agent_targets[2] = model_target\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {} with epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, epsilon))\n",
    "            model_probs[0] = model_probs[0]*model_factor\n",
    "            model_probs[1] = model_probs[1]*model_factor\n",
    "            model_probs[2] = 1 - (model_probs[0]+model_probs[1])\n",
    "            #model_probs = [0,0,1]\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    success_ratio.append(success)\n",
    "    # Update running reward to check condition for solving\n",
    "    epsilon_history.append(epsilon)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    reward_plot.append(episode_reward/timestep)\n",
    "    episode_count += 1\n",
    "    termination_steps.append(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T23:07:34.840178Z",
     "iopub.status.busy": "2022-12-10T23:07:34.839726Z",
     "iopub.status.idle": "2022-12-10T23:07:35.111466Z",
     "shell.execute_reply": "2022-12-10T23:07:35.110714Z",
     "shell.execute_reply.started": "2022-12-10T23:07:34.840151Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotProgress(reward_plot)\n",
    "reward_mean = [np.mean(np.array(reward_plot[i-20:i+1])) for i in range(len(reward_plot))]\n",
    "plotProgress(reward_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T23:07:43.293403Z",
     "iopub.status.busy": "2022-12-10T23:07:43.292999Z",
     "iopub.status.idle": "2022-12-10T23:07:43.577900Z",
     "shell.execute_reply": "2022-12-10T23:07:43.576947Z",
     "shell.execute_reply.started": "2022-12-10T23:07:43.293376Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotProgressTimesteps(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Termination Steps')\n",
    "    plt.title('Total Steps V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressTimesteps(termination_steps)\n",
    "termination_mean = [np.mean(np.array(termination_steps[i-20:i+1])) for i in range(len(termination_steps))]\n",
    "plotProgressTimesteps(termination_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:47:55.207221Z",
     "iopub.status.busy": "2022-12-10T21:47:55.206827Z",
     "iopub.status.idle": "2022-12-10T21:47:55.212283Z",
     "shell.execute_reply": "2022-12-10T21:47:55.211493Z",
     "shell.execute_reply.started": "2022-12-10T21:47:55.207195Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T23:07:56.098133Z",
     "iopub.status.busy": "2022-12-10T23:07:56.097728Z",
     "iopub.status.idle": "2022-12-10T23:08:00.508610Z",
     "shell.execute_reply": "2022-12-10T23:08:00.507854Z",
     "shell.execute_reply.started": "2022-12-10T23:07:56.098109Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed_model.save(\"Pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T10:09:39.216387Z",
     "iopub.status.busy": "2022-12-10T10:09:39.216055Z",
     "iopub.status.idle": "2022-12-10T10:09:39.221123Z",
     "shell.execute_reply": "2022-12-10T10:09:39.220361Z",
     "shell.execute_reply.started": "2022-12-10T10:09:39.216363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T23:20:53.619014Z",
     "iopub.status.busy": "2022-12-10T23:20:53.618565Z",
     "iopub.status.idle": "2022-12-10T23:20:53.630182Z",
     "shell.execute_reply": "2022-12-10T23:20:53.629496Z",
     "shell.execute_reply.started": "2022-12-10T23:20:53.618988Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array(reward_plot)\n",
    "np.savetxt(\"reward_plot_mixed_pretrained.txt\",a)\n",
    "b = np.array(termination_steps)\n",
    "np.savetxt('termination_steps_mixed_pretrained.txt',b)\n",
    "c = np.array(success_ratio)\n",
    "np.savetxt(\"success_mixed_pretrained.txt\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
