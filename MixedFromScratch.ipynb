{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:43:28.709768Z",
     "iopub.status.busy": "2022-12-12T00:43:28.709367Z",
     "iopub.status.idle": "2022-12-12T00:43:34.997492Z",
     "shell.execute_reply": "2022-12-12T00:43:34.996431Z",
     "shell.execute_reply.started": "2022-12-12T00:43:28.709700Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:07.005517Z",
     "iopub.status.busy": "2022-12-12T00:44:07.005072Z",
     "iopub.status.idle": "2022-12-12T00:44:09.054114Z",
     "shell.execute_reply": "2022-12-12T00:44:09.053039Z",
     "shell.execute_reply.started": "2022-12-12T00:44:07.005486Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/tirthankar95/CSCI_7000_FinalProject.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:43:59.503786Z",
     "iopub.status.busy": "2022-12-12T00:43:59.503374Z",
     "iopub.status.idle": "2022-12-12T00:44:00.313224Z",
     "shell.execute_reply": "2022-12-12T00:44:00.312281Z",
     "shell.execute_reply.started": "2022-12-12T00:43:59.503757Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:15.608270Z",
     "iopub.status.busy": "2022-12-12T00:44:15.607844Z",
     "iopub.status.idle": "2022-12-12T00:44:15.614492Z",
     "shell.execute_reply": "2022-12-12T00:44:15.613536Z",
     "shell.execute_reply.started": "2022-12-12T00:44:15.608240Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd CSCI_7000_FinalProject/gym-minigrid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:26.020037Z",
     "iopub.status.busy": "2022-12-12T00:44:26.019658Z",
     "iopub.status.idle": "2022-12-12T00:44:26.023649Z",
     "shell.execute_reply": "2022-12-12T00:44:26.022861Z",
     "shell.execute_reply.started": "2022-12-12T00:44:26.020009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T19:11:25.179249Z",
     "iopub.status.busy": "2022-12-10T19:11:25.178863Z",
     "iopub.status.idle": "2022-12-10T19:11:25.993851Z",
     "shell.execute_reply": "2022-12-10T19:11:25.992811Z",
     "shell.execute_reply.started": "2022-12-10T19:11:25.179225Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:32.312962Z",
     "iopub.status.busy": "2022-12-12T00:44:32.312578Z",
     "iopub.status.idle": "2022-12-12T00:44:35.457553Z",
     "shell.execute_reply": "2022-12-12T00:44:35.456782Z",
     "shell.execute_reply.started": "2022-12-12T00:44:32.312937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:42.461790Z",
     "iopub.status.busy": "2022-12-12T00:44:42.461270Z",
     "iopub.status.idle": "2022-12-12T00:44:45.997890Z",
     "shell.execute_reply": "2022-12-12T00:44:45.997122Z",
     "shell.execute_reply.started": "2022-12-12T00:44:42.461747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:50.372049Z",
     "iopub.status.busy": "2022-12-12T00:44:50.371626Z",
     "iopub.status.idle": "2022-12-12T00:44:50.466515Z",
     "shell.execute_reply": "2022-12-12T00:44:50.465750Z",
     "shell.execute_reply.started": "2022-12-12T00:44:50.372020Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "#env = CrossingEnv(size=6)\n",
    "env = MixedEnv(size=8)\n",
    "env.step_m(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:57.838979Z",
     "iopub.status.busy": "2022-12-12T00:44:57.838592Z",
     "iopub.status.idle": "2022-12-12T00:44:58.909986Z",
     "shell.execute_reply": "2022-12-12T00:44:58.909379Z",
     "shell.execute_reply.started": "2022-12-12T00:44:57.838954Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM,Bidirectional,Dense,Input,Embedding,TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:58.912260Z",
     "iopub.status.busy": "2022-12-12T00:44:58.911342Z",
     "iopub.status.idle": "2022-12-12T00:44:58.915783Z",
     "shell.execute_reply": "2022-12-12T00:44:58.914988Z",
     "shell.execute_reply.started": "2022-12-12T00:44:58.912224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_actions=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:00.948553Z",
     "iopub.status.busy": "2022-12-12T00:45:00.948155Z",
     "iopub.status.idle": "2022-12-12T00:45:00.952492Z",
     "shell.execute_reply": "2022-12-12T00:45:00.951743Z",
     "shell.execute_reply.started": "2022-12-12T00:45:00.948528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plotProgress(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg. reward')\n",
    "    plt.title('Avg Reward Per Step V/S Episodes.')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:05.450799Z",
     "iopub.status.busy": "2022-12-12T00:45:05.450402Z",
     "iopub.status.idle": "2022-12-12T00:45:05.454130Z",
     "shell.execute_reply": "2022-12-12T00:45:05.453479Z",
     "shell.execute_reply.started": "2022-12-12T00:45:05.450770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_offset = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:27.174676Z",
     "iopub.status.busy": "2022-12-12T00:45:27.174280Z",
     "iopub.status.idle": "2022-12-12T00:45:27.179842Z",
     "shell.execute_reply": "2022-12-12T00:45:27.179152Z",
     "shell.execute_reply.started": "2022-12-12T00:45:27.174651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create():\n",
    "    global num_actions\n",
    "    input=Input(shape=((frame_offset+1),147)) # (3,3,7,7) ~ the mini-grid by default returns (3,7,7) image.\n",
    "    model=LSTM(units=256,return_sequences=False)(input)\n",
    "    x1 = Dense(units=256, activation='relu')(model)\n",
    "    x1 = Dense(units=128, activation='relu')(x1)\n",
    "    x1 = Dense(units=64, activation='relu')(x1)\n",
    "    x1 = Dense(units=32, activation='relu')(x1)\n",
    "    x1 = Dense(units=16, activation='relu')(x1)\n",
    "    output=Dense(units=num_actions,activation='linear')(x1)\n",
    "    model=Model(input,output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:28.935891Z",
     "iopub.status.busy": "2022-12-12T00:45:28.935509Z",
     "iopub.status.idle": "2022-12-12T00:45:30.140921Z",
     "shell.execute_reply": "2022-12-12T00:45:30.140151Z",
     "shell.execute_reply.started": "2022-12-12T00:45:28.935865Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:34.451001Z",
     "iopub.status.busy": "2022-12-12T00:55:34.450612Z",
     "iopub.status.idle": "2022-12-12T00:55:35.056769Z",
     "shell.execute_reply": "2022-12-12T00:55:35.056025Z",
     "shell.execute_reply.started": "2022-12-12T00:55:34.450972Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "global num_actions\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 0.8  # Epsilon greedy parameter\n",
    "epsilon_min = 0.2  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 4096  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 5000 #beast 1000\n",
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "# env = DoorKeyEnv(size=8)\n",
    "#env = CrossingEnv(size=6)\n",
    "env = MixedEnv(siz=8)\n",
    "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
    "env.seed(seed)\n",
    "\n",
    "mixed_model=create()\n",
    "mixed_model_target=create()\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer=keras.optimizers.RMSprop()\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "episode_reward_history=[]\n",
    "done_history = []\n",
    "reward_plot = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 100000 #beast 10000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 500000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 200000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 100\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "# We are taking 3 frames in our LSTM\n",
    "#frame_offset=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:35.346253Z",
     "iopub.status.busy": "2022-12-12T00:55:35.345801Z",
     "iopub.status.idle": "2022-12-12T00:55:35.350172Z",
     "shell.execute_reply": "2022-12-12T00:55:35.349463Z",
     "shell.execute_reply.started": "2022-12-12T00:55:35.346211Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "termination_steps=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:35.911253Z",
     "iopub.status.busy": "2022-12-12T00:55:35.910885Z",
     "iopub.status.idle": "2022-12-12T00:55:35.914724Z",
     "shell.execute_reply": "2022-12-12T00:55:35.913950Z",
     "shell.execute_reply.started": "2022-12-12T00:55:35.911227Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_ratio = []\n",
    "epsilon_history = [epsilon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:37.258043Z",
     "iopub.status.busy": "2022-12-12T00:55:37.257654Z",
     "iopub.status.idle": "2022-12-12T00:55:37.263169Z",
     "shell.execute_reply": "2022-12-12T00:55:37.262508Z",
     "shell.execute_reply.started": "2022-12-12T00:55:37.258017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 42\n",
    "np.random.seed(seed=42)\n",
    "def best_action(action_probs):\n",
    "  action = np.argmax(action_probs)\n",
    "  action_probs = np.abs(action_probs - action_probs[action])\n",
    "  possible_actions = []\n",
    "  for i in range(5):\n",
    "    if (action_probs[i] < 0.2):\n",
    "      possible_actions.append(i)\n",
    "  # print(action_probs, possible_actions)\n",
    "  length = len(possible_actions)\n",
    "  action_probs = [1 / length for i in possible_actions]\n",
    "  return np.random.choice(possible_actions, p=action_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:39.013389Z",
     "iopub.status.busy": "2022-12-12T00:55:39.013007Z",
     "iopub.status.idle": "2022-12-12T00:55:39.034027Z",
     "shell.execute_reply": "2022-12-12T00:55:39.033251Z",
     "shell.execute_reply.started": "2022-12-12T00:55:39.013362Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:40.129703Z",
     "iopub.status.busy": "2022-12-12T00:55:40.129341Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noOfEpisodes=100 #beast 100000\n",
    "#while noOfEpisodes:  # Run until solved\n",
    "for _ in tqdm(range(noOfEpisodes)):\n",
    "    #noOfEpisodes-=1\n",
    "    state = np.array(env.reset_m())\n",
    "    episode_reward = 0\n",
    "    success = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            #print(len(state_history))\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):]).reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = mixed_model(state_numpy, training=False)\n",
    "            # Take best action\n",
    "            #action = np.argmax(action_probs[0])\n",
    "            action = best_action(action_probs[0])\n",
    "            #print(action_probs[0])\n",
    "            \n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        #epsilon_history.append(epsilon)\n",
    "        \n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step_m(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "        if(done==True):\n",
    "            success = 1\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history.append(temp_state['image'])\n",
    "        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n",
    "        state_next_history.append(temp_state['image'])\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n",
    "            i=indices[0]\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = mixed_model_target.predict(state_next_sample,verbose=False)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * np.max(\\\n",
    "                future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values.astype('float32')\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            with tf.GradientTape() as tape:    \n",
    "                q_values = mixed_model(state_sample)\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values,q_action)\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, mixed_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, mixed_model.trainable_variables))\n",
    "            \n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            mixed_model_target.set_weights(mixed_model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {} with epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, epsilon))\n",
    "        if frame_count > update_target_network*10:\n",
    "            update_target_network = update_target_network*10\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    success_ratio.append(success)\n",
    "    # Update running reward to check condition for solving\n",
    "    epsilon_history.append(epsilon)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    reward_plot.append(episode_reward/timestep)\n",
    "    episode_count += 1\n",
    "    termination_steps.append(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_model.save(\"MixedFromScratch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:54:59.448622Z",
     "iopub.status.busy": "2022-12-12T00:54:59.448232Z",
     "iopub.status.idle": "2022-12-12T00:54:59.756131Z",
     "shell.execute_reply": "2022-12-12T00:54:59.755462Z",
     "shell.execute_reply.started": "2022-12-12T00:54:59.448598Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotProgress(reward_plot)\n",
    "reward_mean = [np.mean(np.array(reward_plot[i-10:i+1])) for i in range(len(reward_plot))]\n",
    "plotProgress(reward_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:28:05.300608Z",
     "iopub.status.busy": "2022-12-10T20:28:05.300218Z",
     "iopub.status.idle": "2022-12-10T20:28:05.309091Z",
     "shell.execute_reply": "2022-12-10T20:28:05.308308Z",
     "shell.execute_reply.started": "2022-12-10T20:28:05.300582Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a = np.array(reward_plot)\n",
    "np.savetxt(\"reward_plot_doorkey_840.txt\",a)\n",
    "b = np.array(termination_steps)\n",
    "np.savetxt('termination_steps_doorkey_840.txt',b)\n",
    "c = np.array(success_ratio)\n",
    "np.savetxt(\"success_doorkey_840.txt\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:45:00.524839Z",
     "iopub.status.busy": "2022-12-10T20:45:00.524443Z",
     "iopub.status.idle": "2022-12-10T20:45:00.773477Z",
     "shell.execute_reply": "2022-12-10T20:45:00.772801Z",
     "shell.execute_reply.started": "2022-12-10T20:45:00.524812Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotProgressTimesteps(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Termination Steps')\n",
    "    plt.title('Total Steps V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressTimesteps(termination_steps)\n",
    "termination_mean = [np.mean(np.array(termination_steps[i-20:i+1])) for i in range(len(termination_steps))]\n",
    "plotProgressTimesteps(termination_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:11:46.112698Z",
     "iopub.status.busy": "2022-12-10T20:11:46.112302Z",
     "iopub.status.idle": "2022-12-10T20:11:46.241084Z",
     "shell.execute_reply": "2022-12-10T20:11:46.240175Z",
     "shell.execute_reply.started": "2022-12-10T20:11:46.112672Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotProgressSuccess(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Success\\Failure')\n",
    "    plt.title(' Success V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressSuccess(success_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:12:18.468395Z",
     "iopub.status.busy": "2022-12-10T20:12:18.467782Z",
     "iopub.status.idle": "2022-12-10T20:12:18.473154Z",
     "shell.execute_reply": "2022-12-10T20:12:18.472194Z",
     "shell.execute_reply.started": "2022-12-10T20:12:18.468349Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cumulative_reward = [termination_steps[i]*reward_plot[i] for i in range(len(reward_plot))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:12:24.385056Z",
     "iopub.status.busy": "2022-12-10T20:12:24.384668Z",
     "iopub.status.idle": "2022-12-10T20:12:24.529405Z",
     "shell.execute_reply": "2022-12-10T20:12:24.528674Z",
     "shell.execute_reply.started": "2022-12-10T20:12:24.385029Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotProgressSuccess(cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:45:38.184830Z",
     "iopub.status.busy": "2022-12-10T20:45:38.184393Z",
     "iopub.status.idle": "2022-12-10T20:45:38.312677Z",
     "shell.execute_reply": "2022-12-10T20:45:38.311934Z",
     "shell.execute_reply.started": "2022-12-10T20:45:38.184805Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotProgressSuccess(epsilon_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:48:35.015720Z",
     "iopub.status.busy": "2022-12-10T20:48:35.015243Z",
     "iopub.status.idle": "2022-12-10T20:48:35.624560Z",
     "shell.execute_reply": "2022-12-10T20:48:35.623801Z",
     "shell.execute_reply.started": "2022-12-10T20:48:35.015688Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "global num_actions\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 0.8  # Epsilon greedy parameter\n",
    "epsilon_min = 0.2  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 128  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 1000 #beast 1000\n",
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "# env = DoorKeyEnv(size=8)\n",
    "env = CrossingEnv(size=8)\n",
    "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
    "#env.seed(seed)\n",
    "\n",
    "lava_model=create()\n",
    "lava_model_target=create()\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer=keras.optimizers.RMSprop()\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "episode_reward_history=[]\n",
    "done_history = []\n",
    "reward_plot = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 5000 #beast 10000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 100000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 50000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 16\n",
    "# How often to update the target network\n",
    "update_target_network = 1000\n",
    "# Using huber loss for stability\n",
    "# We are taking 3 frames in our LSTM\n",
    "#frame_offset=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:48:40.066549Z",
     "iopub.status.busy": "2022-12-10T20:48:40.066099Z",
     "iopub.status.idle": "2022-12-10T20:48:40.070250Z",
     "shell.execute_reply": "2022-12-10T20:48:40.069499Z",
     "shell.execute_reply.started": "2022-12-10T20:48:40.066523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "termination_steps=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T20:48:40.608234Z",
     "iopub.status.busy": "2022-12-10T20:48:40.607814Z",
     "iopub.status.idle": "2022-12-10T20:48:40.612148Z",
     "shell.execute_reply": "2022-12-10T20:48:40.611299Z",
     "shell.execute_reply.started": "2022-12-10T20:48:40.608208Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_ratio = []\n",
    "termination_steps=[]\n",
    "epsilon_history = [epsilon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:05:30.790240Z",
     "iopub.status.busy": "2022-12-10T21:05:30.789796Z",
     "iopub.status.idle": "2022-12-10T21:28:34.658770Z",
     "shell.execute_reply": "2022-12-10T21:28:34.657903Z",
     "shell.execute_reply.started": "2022-12-10T21:05:30.790199Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noOfEpisodes=1000 #beast 100000\n",
    "#while noOfEpisodes:  # Run until solved\n",
    "for _ in tqdm(range(noOfEpisodes)):\n",
    "    #noOfEpisodes-=1\n",
    "    state = np.array(env.reset_m())\n",
    "    episode_reward = 0\n",
    "    success = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            #print(len(state_history))\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):]).reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = lava_model(state_numpy, training=False)\n",
    "            # Take best action\n",
    "            #action = np.argmax(action_probs[0])\n",
    "            action = best_action(action_probs[0])\n",
    "            #print(action_probs[0])\n",
    "            \n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        #epsilon_history.append(epsilon)\n",
    "        \n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step_m(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "        if(done==True):\n",
    "            success = 1\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history.append(temp_state['image'])\n",
    "        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n",
    "        state_next_history.append(temp_state['image'])\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n",
    "            i=indices[0]\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = lava_model_target.predict(state_next_sample,verbose=False)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * np.max(\\\n",
    "                future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values.astype('float32')\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            with tf.GradientTape() as tape:    \n",
    "                q_values = lava_model(state_sample)\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values,q_action)\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, lava_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, lava_model.trainable_variables))\n",
    "            \n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            lava_model_target.set_weights(lava_model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {} with epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, epsilon))\n",
    "        if frame_count > update_target_network*10:\n",
    "            update_target_network = update_target_network*10\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    success_ratio.append(success)\n",
    "    # Update running reward to check condition for solving\n",
    "    epsilon_history.append(epsilon)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    reward_plot.append(episode_reward/timestep)\n",
    "    episode_count += 1\n",
    "    termination_steps.append(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:29:43.889037Z",
     "iopub.status.busy": "2022-12-10T21:29:43.888647Z",
     "iopub.status.idle": "2022-12-10T21:29:44.311313Z",
     "shell.execute_reply": "2022-12-10T21:29:44.310515Z",
     "shell.execute_reply.started": "2022-12-10T21:29:43.889009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotProgress(reward_plot)\n",
    "reward_mean = [np.mean(np.array(reward_plot[:i+1])) for i in range(len(reward_plot))]\n",
    "plotProgress(reward_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:29:56.163847Z",
     "iopub.status.busy": "2022-12-10T21:29:56.163408Z",
     "iopub.status.idle": "2022-12-10T21:29:56.598210Z",
     "shell.execute_reply": "2022-12-10T21:29:56.596999Z",
     "shell.execute_reply.started": "2022-12-10T21:29:56.163797Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotProgressTimesteps(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Termination Steps')\n",
    "    plt.title('Total Steps V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressTimesteps(termination_steps)\n",
    "termination_mean = [np.mean(np.array(termination_steps[:i+1])) for i in range(len(termination_steps))]\n",
    "plotProgressTimesteps(termination_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:30:27.232112Z",
     "iopub.status.busy": "2022-12-10T21:30:27.231700Z",
     "iopub.status.idle": "2022-12-10T21:30:31.209436Z",
     "shell.execute_reply": "2022-12-10T21:30:31.208698Z",
     "shell.execute_reply.started": "2022-12-10T21:30:27.232086Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lava_model.save('LavaExperts_840')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:30:50.064790Z",
     "iopub.status.busy": "2022-12-10T21:30:50.064395Z",
     "iopub.status.idle": "2022-12-10T21:30:50.082970Z",
     "shell.execute_reply": "2022-12-10T21:30:50.082195Z",
     "shell.execute_reply.started": "2022-12-10T21:30:50.064765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array(reward_plot)\n",
    "np.savetxt(\"reward_plot_lava_840.txt\",a)\n",
    "b = np.array(termination_steps)\n",
    "np.savetxt('termination_steps_lava_840.txt',b)\n",
    "c = np.array(success_ratio)\n",
    "np.savetxt(\"success_lava_840.txt\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T02:30:04.487866Z",
     "iopub.status.busy": "2022-12-10T02:30:04.487456Z",
     "iopub.status.idle": "2022-12-10T02:30:04.492708Z",
     "shell.execute_reply": "2022-12-10T02:30:04.491970Z",
     "shell.execute_reply.started": "2022-12-10T02:30:04.487839Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:30:54.917684Z",
     "iopub.status.busy": "2022-12-10T21:30:54.917281Z",
     "iopub.status.idle": "2022-12-10T21:30:55.517929Z",
     "shell.execute_reply": "2022-12-10T21:30:55.516791Z",
     "shell.execute_reply.started": "2022-12-10T21:30:54.917658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotProgressSuccess(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Success\\Failure')\n",
    "    plt.title(' Success V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressSuccess(success_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:32:30.119686Z",
     "iopub.status.busy": "2022-12-10T21:32:30.119278Z",
     "iopub.status.idle": "2022-12-10T21:32:30.764116Z",
     "shell.execute_reply": "2022-12-10T21:32:30.763325Z",
     "shell.execute_reply.started": "2022-12-10T21:32:30.119660Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "global num_actions\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 0.8  # Epsilon greedy parameter\n",
    "epsilon_min = 0.2  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 4096  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 5000 #beast 1000\n",
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "# env = DoorKeyEnv(size=8)\n",
    "# env = CrossingEnv(size=8)\n",
    "env = MixedEnv(size=8)\n",
    "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
    "#env.seed(seed)\n",
    "\n",
    "mixed_model=create()\n",
    "model_target=create()\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer=keras.optimizers.RMSprop()\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "episode_reward_history=[]\n",
    "done_history = []\n",
    "reward_plot = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 100000 #beast 10000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 500000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 500000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 100\n",
    "# How often to update the target network\n",
    "update_target_network = 100000\n",
    "# Using huber loss for stability\n",
    "# We are taking 3 frames in our LSTM\n",
    "#frame_offset=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:32:33.494243Z",
     "iopub.status.busy": "2022-12-10T21:32:33.493837Z",
     "iopub.status.idle": "2022-12-10T21:32:33.498163Z",
     "shell.execute_reply": "2022-12-10T21:32:33.497249Z",
     "shell.execute_reply.started": "2022-12-10T21:32:33.494217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_factor = 0.9\n",
    "model_probs = [0.333, 0.333, 0.334]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:34:09.767584Z",
     "iopub.status.busy": "2022-12-10T21:34:09.767183Z",
     "iopub.status.idle": "2022-12-10T21:34:14.004703Z",
     "shell.execute_reply": "2022-12-10T21:34:14.003954Z",
     "shell.execute_reply.started": "2022-12-10T21:34:09.767559Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed_agent_targets = []\n",
    "mixed_agent_targets.append(keras.models.load_model(\"DoorKey_840_Epochs400\"))\n",
    "mixed_agent_targets.append(keras.models.load_model(\"LavaExperts_840\"))\n",
    "mixed_agent_targets.append(create())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:34:14.006304Z",
     "iopub.status.busy": "2022-12-10T21:34:14.005985Z",
     "iopub.status.idle": "2022-12-10T21:34:14.010540Z",
     "shell.execute_reply": "2022-12-10T21:34:14.009582Z",
     "shell.execute_reply.started": "2022-12-10T21:34:14.006280Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_ratio = []\n",
    "termination_steps=[]\n",
    "epsilon_history = [epsilon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T22:43:16.476590Z",
     "iopub.status.busy": "2022-12-10T22:43:16.475812Z",
     "iopub.status.idle": "2022-12-10T23:05:21.486566Z",
     "shell.execute_reply": "2022-12-10T23:05:21.485834Z",
     "shell.execute_reply.started": "2022-12-10T22:43:16.476555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noOfEpisodes=100 #beast 100000\n",
    "#while noOfEpisodes:  # Run until solved\n",
    "for _ in tqdm(range(noOfEpisodes)):\n",
    "    #noOfEpisodes-=1\n",
    "    state = np.array(env.reset_m())\n",
    "    episode_reward = 0\n",
    "    success = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            #print(len(state_history))\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):]).reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = mixed_model(state_numpy, training=False)\n",
    "            # Take best action\n",
    "            action = np.argmax(action_probs[0])\n",
    "            #action = best_action(action_probs[0])\n",
    "            #print(action_probs[0])\n",
    "            \n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        #epsilon_history.append(epsilon)\n",
    "        \n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step_m(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "        if(done==True):\n",
    "            success = 1\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history.append(temp_state['image'])\n",
    "        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n",
    "        state_next_history.append(temp_state['image'])\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n",
    "            i=indices[0]\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            #Pick one of the expert or self\n",
    "            target_id = np.random.choice(3,p=model_probs)\n",
    "            model_target = mixed_agent_targets[target_id]\n",
    "            \n",
    "            future_rewards = model_target.predict(state_next_sample,verbose=False)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * np.max(\\\n",
    "                future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values.astype('float32')\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            with tf.GradientTape() as tape:    \n",
    "                q_values = mixed_model(state_sample)\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values,q_action)\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, mixed_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, mixed_model.trainable_variables))\n",
    "            \n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target = mixed_agent_targets[2]\n",
    "            model_target.set_weights(mixed_model.get_weights())\n",
    "            mixed_agent_targets[2] = model_target\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {} with epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, epsilon))\n",
    "            model_probs[0] = model_probs[0]*model_factor\n",
    "            model_probs[1] = model_probs[1]*model_factor\n",
    "            model_probs[2] = 1 - (model_probs[0]+model_probs[1])\n",
    "            #model_probs = [0,0,1]\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    success_ratio.append(success)\n",
    "    # Update running reward to check condition for solving\n",
    "    epsilon_history.append(epsilon)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    reward_plot.append(episode_reward/timestep)\n",
    "    episode_count += 1\n",
    "    termination_steps.append(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T23:07:34.840178Z",
     "iopub.status.busy": "2022-12-10T23:07:34.839726Z",
     "iopub.status.idle": "2022-12-10T23:07:35.111466Z",
     "shell.execute_reply": "2022-12-10T23:07:35.110714Z",
     "shell.execute_reply.started": "2022-12-10T23:07:34.840151Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotProgress(reward_plot)\n",
    "reward_mean = [np.mean(np.array(reward_plot[i-20:i+1])) for i in range(len(reward_plot))]\n",
    "plotProgress(reward_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T23:07:43.293403Z",
     "iopub.status.busy": "2022-12-10T23:07:43.292999Z",
     "iopub.status.idle": "2022-12-10T23:07:43.577900Z",
     "shell.execute_reply": "2022-12-10T23:07:43.576947Z",
     "shell.execute_reply.started": "2022-12-10T23:07:43.293376Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotProgressTimesteps(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Termination Steps')\n",
    "    plt.title('Total Steps V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressTimesteps(termination_steps)\n",
    "termination_mean = [np.mean(np.array(termination_steps[i-20:i+1])) for i in range(len(termination_steps))]\n",
    "plotProgressTimesteps(termination_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:47:55.207221Z",
     "iopub.status.busy": "2022-12-10T21:47:55.206827Z",
     "iopub.status.idle": "2022-12-10T21:47:55.212283Z",
     "shell.execute_reply": "2022-12-10T21:47:55.211493Z",
     "shell.execute_reply.started": "2022-12-10T21:47:55.207195Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T23:07:56.098133Z",
     "iopub.status.busy": "2022-12-10T23:07:56.097728Z",
     "iopub.status.idle": "2022-12-10T23:08:00.508610Z",
     "shell.execute_reply": "2022-12-10T23:08:00.507854Z",
     "shell.execute_reply.started": "2022-12-10T23:07:56.098109Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed_model.save(\"Pretrained_840_Epochs400\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T10:09:39.216387Z",
     "iopub.status.busy": "2022-12-10T10:09:39.216055Z",
     "iopub.status.idle": "2022-12-10T10:09:39.221123Z",
     "shell.execute_reply": "2022-12-10T10:09:39.220361Z",
     "shell.execute_reply.started": "2022-12-10T10:09:39.216363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T23:20:53.619014Z",
     "iopub.status.busy": "2022-12-10T23:20:53.618565Z",
     "iopub.status.idle": "2022-12-10T23:20:53.630182Z",
     "shell.execute_reply": "2022-12-10T23:20:53.629496Z",
     "shell.execute_reply.started": "2022-12-10T23:20:53.618988Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array(reward_plot)\n",
    "np.savetxt(\"reward_plot_mixed_840.txt\",a)\n",
    "b = np.array(termination_steps)\n",
    "np.savetxt('termination_steps_mixed_840.txt',b)\n",
    "c = np.array(success_ratio)\n",
    "np.savetxt(\"success_mixed_840.txt\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T07:48:23.385388Z",
     "iopub.status.busy": "2022-12-11T07:48:23.385017Z",
     "iopub.status.idle": "2022-12-11T07:48:23.656546Z",
     "shell.execute_reply": "2022-12-11T07:48:23.655795Z",
     "shell.execute_reply.started": "2022-12-11T07:48:23.385363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "global num_actions\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 0.8  # Epsilon greedy parameter\n",
    "epsilon_min = 0.2  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 4096  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 5000 #beast 1000\n",
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "env = DoorKeyEnv(size=8)\n",
    "env = CrossingEnv(size=8)\n",
    "env = MixedEnv(size=8)\n",
    "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
    "env.seed(seed)\n",
    "\n",
    "mixed_model=create()\n",
    "#model_target=create()\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer=keras.optimizers.RMSprop()\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "episode_reward_history=[]\n",
    "done_history = []\n",
    "reward_plot = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 100000 #beast 10000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 500000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 500000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 100\n",
    "# How often to update the target network\n",
    "update_target_network = 100000\n",
    "# Using huber loss for stability\n",
    "# We are taking 3 frames in our LSTM\n",
    "#frame_offset=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T07:48:26.009450Z",
     "iopub.status.busy": "2022-12-11T07:48:26.009065Z",
     "iopub.status.idle": "2022-12-11T07:48:30.134633Z",
     "shell.execute_reply": "2022-12-11T07:48:30.133776Z",
     "shell.execute_reply.started": "2022-12-11T07:48:26.009425Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mixed_agent_targets = []\n",
    "mixed_agent_targets.append(keras.models.load_model(\"DoorKey\"))\n",
    "mixed_agent_targets.append(keras.models.load_model(\"LavaExpertsSaturn2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T07:48:39.442438Z",
     "iopub.status.busy": "2022-12-11T07:48:39.442034Z",
     "iopub.status.idle": "2022-12-11T07:54:21.268923Z",
     "shell.execute_reply": "2022-12-11T07:54:21.268172Z",
     "shell.execute_reply.started": "2022-12-11T07:48:39.442413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_steps = 5000\n",
    "episodes = 10\n",
    "experts_rewards = []\n",
    "for e in range(len(mixed_agent_targets)):\n",
    "    avg_reward = 0\n",
    "    for i in range(episodes):\n",
    "        state = np.array(env.reset_m())\n",
    "        episode_reward = 0\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history = [temp_state['image']]\n",
    "        for i in range(frame_offset):\n",
    "            action = np.random.choice(5)\n",
    "            state, reward, done, _ = env.step_m(action)\n",
    "            state_numpy = np.array(state)\n",
    "            temp_state = dict(state_numpy.item(0))\n",
    "            state_history.append(temp_state['image'])\n",
    "        #print(np.array(state_history[-8:]).shape)\n",
    "        for j in range(max_steps):\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):])\n",
    "            #print(state_numpy.shape)\n",
    "            state_numpy = state_numpy.reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = mixed_agent_targets[e](state_numpy, training=False)\n",
    "            action = np.argmax(action_probs[0])\n",
    "            state, reward, done, _ = env.step_m(action)\n",
    "            state_numpy = np.array(state)\n",
    "            temp_state = dict(state_numpy.item(0))\n",
    "            del state_history[:1]\n",
    "            state_history.append(temp_state['image'])\n",
    "            # state_history.append(state)\n",
    "            #print(done,action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        episode_avg_reward = episode_reward/j\n",
    "        avg_reward += episode_avg_reward\n",
    "        print(e,episode_avg_reward,j,episode_reward)\n",
    "    experts_rewards.append(avg_reward/episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T07:54:21.270924Z",
     "iopub.status.busy": "2022-12-11T07:54:21.270370Z",
     "iopub.status.idle": "2022-12-11T07:54:21.278674Z",
     "shell.execute_reply": "2022-12-11T07:54:21.277736Z",
     "shell.execute_reply.started": "2022-12-11T07:54:21.270897Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "experts_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(arr):\n",
    "  for i in range(len(arr)):\n",
    "    print(np.exp(arr[i])/np.sum(np.exp(arr)))\n",
    "weights = softmax(experts_rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
