{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALAIPri6MIxa",
        "outputId": "1dbf14e7-8865-4268-b08a-dfcac1c25baf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'CSCI_7000_FinalProject'...\n",
            "remote: Enumerating objects: 363, done.\u001b[K\n",
            "remote: Counting objects: 100% (363/363), done.\u001b[K\n",
            "remote: Compressing objects: 100% (273/273), done.\u001b[K\n",
            "remote: Total 363 (delta 129), reused 315 (delta 81), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (363/363), 19.57 MiB | 16.91 MiB/s, done.\n",
            "Resolving deltas: 100% (129/129), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/tirthankar95/CSCI_7000_FinalProject.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD0W_MbuMq2S",
        "outputId": "014538fb-bd8e-44b2-ab14-e07f8839c98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/CSCI_7000_FinalProject/Ajay/gym-minigrid\n"
          ]
        }
      ],
      "source": [
        "%cd CSCI_7000_FinalProject/Ajay/gym-minigrid/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_AVuwNaMwEU",
        "outputId": "7e8f67de-a826-4789-f9e6-a9ff9c4e1461"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/CSCI_7000_FinalProject/Ajay/gym-minigrid\n",
            "Requirement already satisfied: gym>=0.9.6 in /usr/local/lib/python3.8/dist-packages (from gym-minigrid==0.0.5) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from gym-minigrid==0.0.5) (1.21.6)\n",
            "Collecting pyqt5>=5.10.1\n",
            "  Downloading PyQt5-5.15.7-cp37-abi3-manylinux1_x86_64.whl (8.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.4 MB 38.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.9.6->gym-minigrid==0.0.5) (1.5.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym>=0.9.6->gym-minigrid==0.0.5) (4.13.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym>=0.9.6->gym-minigrid==0.0.5) (0.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym>=0.9.6->gym-minigrid==0.0.5) (3.10.0)\n",
            "Collecting PyQt5-sip<13,>=12.11\n",
            "  Downloading PyQt5_sip-12.11.0-cp38-cp38-manylinux1_x86_64.whl (361 kB)\n",
            "\u001b[K     |████████████████████████████████| 361 kB 69.0 MB/s \n",
            "\u001b[?25hCollecting PyQt5-Qt5>=5.15.0\n",
            "  Downloading PyQt5_Qt5-5.15.2-py3-none-manylinux2014_x86_64.whl (59.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.9 MB 1.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: PyQt5-sip, PyQt5-Qt5, pyqt5, gym-minigrid\n",
            "  Running setup.py develop for gym-minigrid\n",
            "Successfully installed PyQt5-Qt5-5.15.2 PyQt5-sip-12.11.0 gym-minigrid-0.0.5 pyqt5-5.15.7\n"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHRf-tShMzMM",
        "outputId": "4ccb1f81-cf12-46c7-858e-440981970be9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/content/CSCI_7000_FinalProject/Ajay/gym-minigrid/gym_minigrid/roomgrid.py:300: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if front_cell is None or front_cell.type is 'wall':\n",
            "/content/CSCI_7000_FinalProject/Ajay/gym-minigrid/gym_minigrid/minigrid.py:663: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  mask = np.zeros(shape=(grid.width, grid.height), dtype=np.bool)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "({'image': array([[[0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [2, 5, 0],\n",
              "          [2, 5, 0],\n",
              "          [2, 5, 0],\n",
              "          [4, 4, 2],\n",
              "          [2, 5, 0]],\n",
              "  \n",
              "         [[0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [2, 5, 0],\n",
              "          [5, 4, 0],\n",
              "          [1, 0, 0],\n",
              "          [1, 0, 0],\n",
              "          [1, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [2, 5, 0],\n",
              "          [2, 5, 0],\n",
              "          [2, 5, 0],\n",
              "          [2, 5, 0],\n",
              "          [2, 5, 0]],\n",
              "  \n",
              "         [[0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0]],\n",
              "  \n",
              "         [[0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0],\n",
              "          [0, 0, 0]]], dtype=uint8),\n",
              "  'direction': 1,\n",
              "  'mission': 'use the key to open the door and then get to the goal'},\n",
              " -1,\n",
              " False,\n",
              " (1, 1))"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from gym_minigrid.envs.doorkey import *\n",
        "env = DoorKeyEnv(size=6)\n",
        "env.step_m(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Ufnk-Hx0M5wh"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import gym_minigrid\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.models import Sequential,Model\n",
        "from keras.layers import LSTM,Bidirectional,Dense,Input,Embedding,TimeDistributed\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "d12pNDSOM-K-"
      },
      "outputs": [],
      "source": [
        "num_actions=5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2yozA0MIM__e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def plotProgress(reward_plot):\n",
        "    plt.plot(reward_plot)\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Avg. reward')\n",
        "    plt.title('Avg Reward Per Step V/S Episodes.')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "A4xDRnJ-NB1i"
      },
      "outputs": [],
      "source": [
        "def create():\n",
        "    global num_actions\n",
        "    input=Input(shape=(3,147)) # (3,3,7,7) ~ the mini-grid by default returns (3,7,7) image.\n",
        "    model=LSTM(units=32,return_sequences=False)(input)\n",
        "    output=Dense(units=num_actions,activation='relu')(model)\n",
        "    model=Model(input,output)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LW0DguegNDVH"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "global num_actions\n",
        "# Configuration paramaters for the whole setup\n",
        "seed = 42\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "epsilon = 1.0  # Epsilon greedy parameter\n",
        "epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
        "epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
        "epsilon_interval = (\n",
        "    epsilon_max - epsilon_min\n",
        ")  # Rate at which to reduce chance of random action being taken\n",
        "batch_size = 4098  # Size of batch taken from replay buffer\n",
        "max_steps_per_episode = 5000 #beast 1000\n",
        "from gym_minigrid.envs.doorkey import *\n",
        "env = DoorKeyEnv(size=6)\n",
        "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
        "env.seed(seed)\n",
        "\n",
        "model=create()\n",
        "model_target=create()\n",
        "loss_function = keras.losses.MeanSquaredError()\n",
        "optimizer=keras.optimizers.RMSprop()\n",
        "\n",
        "# Experience replay buffers\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "episode_reward_history=[]\n",
        "done_history = []\n",
        "reward_plot = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "# Number of frames to take random action and observe output\n",
        "epsilon_random_frames = 1000 #beast 10000\n",
        "# Number of frames for exploration\n",
        "epsilon_greedy_frames = 1000000.0\n",
        "# Maximum replay length\n",
        "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
        "max_memory_length = 100000\n",
        "# Train the model after 4 actions\n",
        "update_after_actions = 100\n",
        "# How often to update the target network\n",
        "update_target_network = 10000\n",
        "# Using huber loss for stability\n",
        "# We are taking 3 frames in our LSTM\n",
        "frame_offset=2\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-j58-TWNHBt",
        "outputId": "ebeed344-6f20-4fd6-8978-5a6b61adbc30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 10/1000 [00:21<50:21,  3.05s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1964.20 at episode 10, frame count 10000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 20/1000 [01:08<47:35,  2.91s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2455.35 at episode 20, frame count 20000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  3%|▎         | 33/1000 [01:46<47:46,  2.96s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2196.27 at episode 33, frame count 30000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 41/1000 [02:28<1:33:45,  5.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2401.37 at episode 41, frame count 40000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▍         | 49/1000 [03:13<1:09:45,  4.40s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2543.42 at episode 48, frame count 50000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▋         | 63/1000 [03:54<56:43,  3.63s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2384.65 at episode 63, frame count 60000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 78/1000 [04:38<40:45,  2.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2233.32 at episode 78, frame count 70000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 89/1000 [05:18<54:11,  3.57s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2217.19 at episode 89, frame count 80000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 11%|█         | 106/1000 [05:59<20:58,  1.41s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2073.55 at episode 106, frame count 90000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 120/1000 [06:47<51:44,  3.53s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2054.68 at episode 120, frame count 100000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 13%|█▎        | 132/1000 [07:31<57:06,  3.95s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2034.72 at episode 132, frame count 110000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 140/1000 [08:16<1:16:14,  5.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2083.84 at episode 140, frame count 120000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 153/1000 [09:06<38:08,  2.70s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2069.81 at episode 153, frame count 130000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 17%|█▋        | 169/1000 [09:56<40:29,  2.92s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -2016.17 at episode 169, frame count 140000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 19%|█▊        | 186/1000 [10:46<38:00,  2.80s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1954.85 at episode 186, frame count 150000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 201/1000 [11:27<40:04,  3.01s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1904.03 at episode 201, frame count 160000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 215/1000 [12:23<34:51,  2.66s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1899.88 at episode 215, frame count 170000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 23%|██▎       | 228/1000 [13:17<56:58,  4.43s/it]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1900.40 at episode 228, frame count 180000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 242/1000 [14:07<43:51,  3.47s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1883.01 at episode 242, frame count 190000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 256/1000 [14:57<35:32,  2.87s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1867.63 at episode 256, frame count 200000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 27%|██▋       | 272/1000 [15:53<38:47,  3.20s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1846.37 at episode 271, frame count 210000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 29%|██▊       | 286/1000 [16:42<40:48,  3.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1828.72 at episode 286, frame count 220000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 300/1000 [17:32<51:37,  4.43s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1812.47 at episode 300, frame count 230000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 311/1000 [18:25<44:34,  3.88s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1820.94 at episode 311, frame count 240000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 33%|███▎      | 326/1000 [19:18<40:57,  3.65s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1803.55 at episode 326, frame count 250000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 340/1000 [20:16<35:04,  3.19s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1798.00 at episode 340, frame count 260000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 358/1000 [21:09<28:32,  2.67s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1766.51 at episode 358, frame count 270000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 372/1000 [22:05<32:06,  3.07s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1761.20 at episode 371, frame count 280000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 39%|███▊      | 387/1000 [22:57<33:32,  3.28s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1742.48 at episode 387, frame count 290000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|███▉      | 399/1000 [23:53<43:45,  4.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "running reward: -1744.88 at episode 399, frame count 300000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 404/1000 [24:16<40:12,  4.05s/it]"
          ]
        }
      ],
      "source": [
        "noOfEpisodes=1000 #beast 100000\n",
        "#while noOfEpisodes:  # Run until solved\n",
        "for _ in tqdm(range(noOfEpisodes)):\n",
        "    #noOfEpisodes-=1\n",
        "    state = np.array(env.reset_m())\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        # env.render(); Adding this line would show the attempts\n",
        "        # of the agent in a pop up window.\n",
        "        frame_count += 1\n",
        "\n",
        "        # Use epsilon-greedy for exploration\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
        "            # Take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # Predict action Q-values\n",
        "            # From environment state\n",
        "            state_numpy = np.array(state_history[-3:]).reshape(3,147)\n",
        "            state_numpy = np.array([state_numpy])\n",
        "            action_probs = model(state_numpy, training=False)\n",
        "            # Take best action\n",
        "            action = np.argmax(action_probs[0])\n",
        "\n",
        "        # Decay probability of taking random action\n",
        "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
        "        epsilon = max(epsilon, epsilon_min)\n",
        "\n",
        "        # Apply the sampled action in our environment\n",
        "        state_next, reward, done, _ = env.step_m(action)\n",
        "        state_next = np.array(state_next)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Save actions and states in replay buffer\n",
        "        action_history.append(action)\n",
        "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
        "        state_history.append(temp_state['image'])\n",
        "        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n",
        "        state_next_history.append(temp_state['image'])\n",
        "        done_history.append(done)\n",
        "        rewards_history.append(reward)\n",
        "        state = state_next\n",
        "\n",
        "        # Update every fourth frame and once batch size is over 32\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "            # Get indices of samples for replay buffers\n",
        "            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n",
        "            i=indices[0]\n",
        "            # Using list comprehension to sample from replay buffer\n",
        "            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape(3,147) for i in indices])\n",
        "            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape(3,147) for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            # Build the updated Q-values for the sampled future states\n",
        "            # Use the target model for stability\n",
        "            future_rewards = model_target.predict(state_next_sample,verbose=False)\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            updated_q_values = rewards_sample + gamma * np.max(\\\n",
        "                future_rewards, axis=1)\n",
        "            updated_q_values = updated_q_values.astype('float32')\n",
        "            # Create a mask so we only calculate loss on the updated Q-values\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "            with tf.GradientTape() as tape:    \n",
        "                q_values = model(state_sample)\n",
        "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                loss = loss_function(updated_q_values,q_action)\n",
        "            # Backpropagation\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            \n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            # update the the target network with new weights\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # Log details\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count))\n",
        "\n",
        "        # Limit the state and reward history\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "# Update running reward to check condition for solving\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "    reward_plot.append(episode_reward/timestep)\n",
        "    episode_count += 1\n",
        "plotProgress(reward_plot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ln6JyiT0NPcU"
      },
      "outputs": [],
      "source": [
        "model.save('Expert1.ml')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "8dbd230d554e8d7f14cfdcdafae9d0c6157845c7162401b2bf4626097cee1420"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
