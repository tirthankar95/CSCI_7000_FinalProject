{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:00:41.799789Z",
     "iopub.status.busy": "2022-12-12T03:00:41.799197Z",
     "iopub.status.idle": "2022-12-12T03:00:48.171922Z",
     "shell.execute_reply": "2022-12-12T03:00:48.171224Z",
     "shell.execute_reply.started": "2022-12-12T03:00:41.799723Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:07.005517Z",
     "iopub.status.busy": "2022-12-12T00:44:07.005072Z",
     "iopub.status.idle": "2022-12-12T00:44:09.054114Z",
     "shell.execute_reply": "2022-12-12T00:44:09.053039Z",
     "shell.execute_reply.started": "2022-12-12T00:44:07.005486Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/tirthankar95/CSCI_7000_FinalProject.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:43:59.503786Z",
     "iopub.status.busy": "2022-12-12T00:43:59.503374Z",
     "iopub.status.idle": "2022-12-12T00:44:00.313224Z",
     "shell.execute_reply": "2022-12-12T00:44:00.312281Z",
     "shell.execute_reply.started": "2022-12-12T00:43:59.503757Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:00:48.178314Z",
     "iopub.status.busy": "2022-12-12T03:00:48.178121Z",
     "iopub.status.idle": "2022-12-12T03:00:48.183419Z",
     "shell.execute_reply": "2022-12-12T03:00:48.182710Z",
     "shell.execute_reply.started": "2022-12-12T03:00:48.178294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cd CSCI_7000_FinalProject/gym-minigrid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:00:48.184975Z",
     "iopub.status.busy": "2022-12-12T03:00:48.184723Z",
     "iopub.status.idle": "2022-12-12T03:00:48.188576Z",
     "shell.execute_reply": "2022-12-12T03:00:48.187746Z",
     "shell.execute_reply.started": "2022-12-12T03:00:48.184955Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T19:11:25.179249Z",
     "iopub.status.busy": "2022-12-10T19:11:25.178863Z",
     "iopub.status.idle": "2022-12-10T19:11:25.993851Z",
     "shell.execute_reply": "2022-12-10T19:11:25.992811Z",
     "shell.execute_reply.started": "2022-12-10T19:11:25.179225Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:00:50.919884Z",
     "iopub.status.busy": "2022-12-12T03:00:50.919517Z",
     "iopub.status.idle": "2022-12-12T03:00:54.016203Z",
     "shell.execute_reply": "2022-12-12T03:00:54.015296Z",
     "shell.execute_reply.started": "2022-12-12T03:00:50.919853Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:00:56.941367Z",
     "iopub.status.busy": "2022-12-12T03:00:56.940921Z",
     "iopub.status.idle": "2022-12-12T03:01:00.480534Z",
     "shell.execute_reply": "2022-12-12T03:01:00.479631Z",
     "shell.execute_reply.started": "2022-12-12T03:00:56.941337Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:08.185373Z",
     "iopub.status.busy": "2022-12-12T03:01:08.184947Z",
     "iopub.status.idle": "2022-12-12T03:01:08.267986Z",
     "shell.execute_reply": "2022-12-12T03:01:08.267322Z",
     "shell.execute_reply.started": "2022-12-12T03:01:08.185341Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "#env = CrossingEnv(size=6)\n",
    "env = MixedEnv(size=8)\n",
    "env.step_m(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:14.669790Z",
     "iopub.status.busy": "2022-12-12T03:01:14.669399Z",
     "iopub.status.idle": "2022-12-12T03:01:15.667043Z",
     "shell.execute_reply": "2022-12-12T03:01:15.666421Z",
     "shell.execute_reply.started": "2022-12-12T03:01:14.669765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM,Bidirectional,Dense,Input,Embedding,TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:15.668736Z",
     "iopub.status.busy": "2022-12-12T03:01:15.668274Z",
     "iopub.status.idle": "2022-12-12T03:01:15.671831Z",
     "shell.execute_reply": "2022-12-12T03:01:15.671212Z",
     "shell.execute_reply.started": "2022-12-12T03:01:15.668711Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_actions=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:16.453057Z",
     "iopub.status.busy": "2022-12-12T03:01:16.452675Z",
     "iopub.status.idle": "2022-12-12T03:01:16.457175Z",
     "shell.execute_reply": "2022-12-12T03:01:16.456366Z",
     "shell.execute_reply.started": "2022-12-12T03:01:16.453032Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plotProgress(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg. reward')\n",
    "    plt.title('Avg Reward Per Step V/S Episodes.')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:18.735191Z",
     "iopub.status.busy": "2022-12-12T03:01:18.734803Z",
     "iopub.status.idle": "2022-12-12T03:01:18.738405Z",
     "shell.execute_reply": "2022-12-12T03:01:18.737740Z",
     "shell.execute_reply.started": "2022-12-12T03:01:18.735166Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_offset = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:19.456761Z",
     "iopub.status.busy": "2022-12-12T03:01:19.456355Z",
     "iopub.status.idle": "2022-12-12T03:01:19.462198Z",
     "shell.execute_reply": "2022-12-12T03:01:19.461413Z",
     "shell.execute_reply.started": "2022-12-12T03:01:19.456734Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create():\n",
    "    global num_actions\n",
    "    input=Input(shape=((frame_offset+1),147)) # (3,3,7,7) ~ the mini-grid by default returns (3,7,7) image.\n",
    "    model=LSTM(units=256,return_sequences=False)(input)\n",
    "    x1 = Dense(units=256, activation='relu')(model)\n",
    "    x1 = Dense(units=128, activation='relu')(x1)\n",
    "    x1 = Dense(units=64, activation='relu')(x1)\n",
    "    x1 = Dense(units=32, activation='relu')(x1)\n",
    "    x1 = Dense(units=16, activation='relu')(x1)\n",
    "    output=Dense(units=num_actions,activation='linear')(x1)\n",
    "    model=Model(input,output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:20.510808Z",
     "iopub.status.busy": "2022-12-12T03:01:20.510434Z",
     "iopub.status.idle": "2022-12-12T03:01:21.800339Z",
     "shell.execute_reply": "2022-12-12T03:01:21.799556Z",
     "shell.execute_reply.started": "2022-12-12T03:01:20.510781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:46.402282Z",
     "iopub.status.busy": "2022-12-12T03:01:46.401878Z",
     "iopub.status.idle": "2022-12-12T03:01:46.897204Z",
     "shell.execute_reply": "2022-12-12T03:01:46.896596Z",
     "shell.execute_reply.started": "2022-12-12T03:01:46.402254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "global num_actions\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 0.8  # Epsilon greedy parameter\n",
    "epsilon_min = 0.2  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 128  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 1000 #beast 1000\n",
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "# env = DoorKeyEnv(size=8)\n",
    "env = CrossingEnv(size=8)\n",
    "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
    "#env.seed(seed)\n",
    "\n",
    "lava_model=create()\n",
    "lava_model_target=create()\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer=keras.optimizers.RMSprop()\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "episode_reward_history=[]\n",
    "done_history = []\n",
    "reward_plot = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 5000 #beast 10000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 100000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 100000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 16\n",
    "# How often to update the target network\n",
    "update_target_network = 1000\n",
    "# Using huber loss for stability\n",
    "# We are taking 3 frames in our LSTM\n",
    "#frame_offset=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:47.598155Z",
     "iopub.status.busy": "2022-12-12T03:01:47.597764Z",
     "iopub.status.idle": "2022-12-12T03:01:47.601667Z",
     "shell.execute_reply": "2022-12-12T03:01:47.600936Z",
     "shell.execute_reply.started": "2022-12-12T03:01:47.598127Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "termination_steps=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:48.318433Z",
     "iopub.status.busy": "2022-12-12T03:01:48.318040Z",
     "iopub.status.idle": "2022-12-12T03:01:48.322165Z",
     "shell.execute_reply": "2022-12-12T03:01:48.321506Z",
     "shell.execute_reply.started": "2022-12-12T03:01:48.318407Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_ratio = []\n",
    "termination_steps=[]\n",
    "epsilon_history = [epsilon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:01:49.379805Z",
     "iopub.status.busy": "2022-12-12T03:01:49.379397Z",
     "iopub.status.idle": "2022-12-12T03:01:49.386475Z",
     "shell.execute_reply": "2022-12-12T03:01:49.385556Z",
     "shell.execute_reply.started": "2022-12-12T03:01:49.379776Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 42\n",
    "np.random.seed(seed=42)\n",
    "def best_action(action_probs):\n",
    "  action = np.argmax(action_probs)\n",
    "  action_probs = np.abs(action_probs - action_probs[action])\n",
    "  possible_actions = []\n",
    "  for i in range(5):\n",
    "    if (action_probs[i] < 0.2):\n",
    "      possible_actions.append(i)\n",
    "  # print(action_probs, possible_actions)\n",
    "  length = len(possible_actions)\n",
    "  action_probs = [1 / length for i in possible_actions]\n",
    "  return np.random.choice(possible_actions, p=action_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:55:55.031123Z",
     "iopub.status.busy": "2022-12-12T03:55:55.030705Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "noOfEpisodes=1000 #beast 100000\n",
    "#while noOfEpisodes:  # Run until solved\n",
    "for _ in tqdm(range(noOfEpisodes)):\n",
    "    #noOfEpisodes-=1\n",
    "    state = np.array(env.reset_m())\n",
    "    episode_reward = 0\n",
    "    success = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            #print(len(state_history))\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):]).reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = lava_model(state_numpy, training=False)\n",
    "            # Take best action\n",
    "            #action = np.argmax(action_probs[0])\n",
    "            action = best_action(action_probs[0])\n",
    "            #print(action_probs[0])\n",
    "            \n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        #epsilon_history.append(epsilon)\n",
    "        \n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step_m(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "        if(done==True):\n",
    "            success = 1\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history.append(temp_state['image'])\n",
    "        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n",
    "        state_next_history.append(temp_state['image'])\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n",
    "            i=indices[0]\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = lava_model_target.predict(state_next_sample,verbose=False)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * np.max(\\\n",
    "                future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values.astype('float32')\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            with tf.GradientTape() as tape:    \n",
    "                q_values = lava_model(state_sample)\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values,q_action)\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, lava_model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, lava_model.trainable_variables))\n",
    "            \n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            lava_model_target.set_weights(lava_model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {} with epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, epsilon))\n",
    "        if frame_count > update_target_network*10:\n",
    "            update_target_network = update_target_network*10\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    success_ratio.append(success)\n",
    "    # Update running reward to check condition for solving\n",
    "    epsilon_history.append(epsilon)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    reward_plot.append(episode_reward/timestep)\n",
    "    episode_count += 1\n",
    "    termination_steps.append(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:54:35.204817Z",
     "iopub.status.busy": "2022-12-12T03:54:35.204431Z",
     "iopub.status.idle": "2022-12-12T03:54:35.559699Z",
     "shell.execute_reply": "2022-12-12T03:54:35.559043Z",
     "shell.execute_reply.started": "2022-12-12T03:54:35.204789Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plotProgress(reward_plot)\n",
    "reward_mean = [np.mean(np.array(reward_plot[:i+1])) for i in range(len(reward_plot))]\n",
    "plotProgress(reward_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:54:41.799713Z",
     "iopub.status.busy": "2022-12-12T03:54:41.799329Z",
     "iopub.status.idle": "2022-12-12T03:54:42.187057Z",
     "shell.execute_reply": "2022-12-12T03:54:42.186144Z",
     "shell.execute_reply.started": "2022-12-12T03:54:41.799684Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotProgressTimesteps(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Termination Steps')\n",
    "    plt.title('Total Steps V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressTimesteps(termination_steps)\n",
    "termination_mean = [np.mean(np.array(termination_steps[:i+1])) for i in range(len(termination_steps))]\n",
    "plotProgressTimesteps(termination_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T03:55:06.026215Z",
     "iopub.status.busy": "2022-12-12T03:55:06.025809Z",
     "iopub.status.idle": "2022-12-12T03:55:09.377260Z",
     "shell.execute_reply": "2022-12-12T03:55:09.376516Z",
     "shell.execute_reply.started": "2022-12-12T03:55:06.026189Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lava_model.save('Lava_Experts2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:30:50.064790Z",
     "iopub.status.busy": "2022-12-10T21:30:50.064395Z",
     "iopub.status.idle": "2022-12-10T21:30:50.082970Z",
     "shell.execute_reply": "2022-12-10T21:30:50.082195Z",
     "shell.execute_reply.started": "2022-12-10T21:30:50.064765Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = np.array(reward_plot)\n",
    "np.savetxt(\"reward_plot_lava.txt\",a)\n",
    "b = np.array(termination_steps)\n",
    "np.savetxt('termination_steps_lava.txt',b)\n",
    "c = np.array(success_ratio)\n",
    "np.savetxt(\"success_lava.txt\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T02:30:04.487866Z",
     "iopub.status.busy": "2022-12-10T02:30:04.487456Z",
     "iopub.status.idle": "2022-12-10T02:30:04.492708Z",
     "shell.execute_reply": "2022-12-10T02:30:04.491970Z",
     "shell.execute_reply.started": "2022-12-10T02:30:04.487839Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "update_target_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T21:30:54.917684Z",
     "iopub.status.busy": "2022-12-10T21:30:54.917281Z",
     "iopub.status.idle": "2022-12-10T21:30:55.517929Z",
     "shell.execute_reply": "2022-12-10T21:30:55.516791Z",
     "shell.execute_reply.started": "2022-12-10T21:30:54.917658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plotProgressSuccess(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Success\\Failure')\n",
    "    plt.title(' Success V/S Episodes.')\n",
    "    plt.show()\n",
    "\n",
    "plotProgressSuccess(success_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
