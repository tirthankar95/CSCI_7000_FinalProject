{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:43:28.709768Z",
     "iopub.status.busy": "2022-12-12T00:43:28.709367Z",
     "iopub.status.idle": "2022-12-12T00:43:34.997492Z",
     "shell.execute_reply": "2022-12-12T00:43:34.996431Z",
     "shell.execute_reply.started": "2022-12-12T00:43:28.709700Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 00:43:28.847744: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-12 00:43:34.980319: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:43:34.986640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:43:34.987237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:07.005517Z",
     "iopub.status.busy": "2022-12-12T00:44:07.005072Z",
     "iopub.status.idle": "2022-12-12T00:44:09.054114Z",
     "shell.execute_reply": "2022-12-12T00:44:09.053039Z",
     "shell.execute_reply.started": "2022-12-12T00:44:07.005486Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CSCI_7000_FinalProject'...\n",
      "remote: Enumerating objects: 687, done.\u001b[K\n",
      "remote: Counting objects: 100% (210/210), done.\u001b[K\n",
      "remote: Compressing objects: 100% (140/140), done.\u001b[K\n",
      "remote: Total 687 (delta 88), reused 173 (delta 57), pack-reused 477\u001b[K\n",
      "Receiving objects: 100% (687/687), 26.90 MiB | 46.07 MiB/s, done.\n",
      "Resolving deltas: 100% (293/293), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/tirthankar95/CSCI_7000_FinalProject.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:43:59.503786Z",
     "iopub.status.busy": "2022-12-12T00:43:59.503374Z",
     "iopub.status.idle": "2022-12-12T00:44:00.313224Z",
     "shell.execute_reply": "2022-12-12T00:44:00.312281Z",
     "shell.execute_reply.started": "2022-12-12T00:43:59.503757Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:15.608270Z",
     "iopub.status.busy": "2022-12-12T00:44:15.607844Z",
     "iopub.status.idle": "2022-12-12T00:44:15.614492Z",
     "shell.execute_reply": "2022-12-12T00:44:15.613536Z",
     "shell.execute_reply.started": "2022-12-12T00:44:15.608240Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/examples/examples/tensorflow/CSCI_7000_FinalProject/gym-minigrid\n"
     ]
    }
   ],
   "source": [
    "%cd CSCI_7000_FinalProject/gym-minigrid/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:26.020037Z",
     "iopub.status.busy": "2022-12-12T00:44:26.019658Z",
     "iopub.status.idle": "2022-12-12T00:44:26.023649Z",
     "shell.execute_reply": "2022-12-12T00:44:26.022861Z",
     "shell.execute_reply.started": "2022-12-12T00:44:26.020009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-10T19:11:25.179249Z",
     "iopub.status.busy": "2022-12-10T19:11:25.178863Z",
     "iopub.status.idle": "2022-12-10T19:11:25.993851Z",
     "shell.execute_reply": "2022-12-10T19:11:25.992811Z",
     "shell.execute_reply.started": "2022-12-10T19:11:25.179225Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:32.312962Z",
     "iopub.status.busy": "2022-12-12T00:44:32.312578Z",
     "iopub.status.idle": "2022-12-12T00:44:35.457553Z",
     "shell.execute_reply": "2022-12-12T00:44:35.456782Z",
     "shell.execute_reply.started": "2022-12-12T00:44:32.312937Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.64.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:42.461790Z",
     "iopub.status.busy": "2022-12-12T00:44:42.461270Z",
     "iopub.status.idle": "2022-12-12T00:44:45.997890Z",
     "shell.execute_reply": "2022-12-12T00:44:45.997122Z",
     "shell.execute_reply.started": "2022-12-12T00:44:42.461747Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/jovyan/examples/examples/tensorflow/CSCI_7000_FinalProject/gym-minigrid\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gym==0.25.2\n",
      "  Using cached gym-0.25.2-py3-none-any.whl\n",
      "Requirement already satisfied: numpy==1.21.6 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gym-minigrid==0.0.5) (1.21.6)\n",
      "Requirement already satisfied: pyqt5>=5.10.1 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gym-minigrid==0.0.5) (5.15.7)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gym==0.25.2->gym-minigrid==0.0.5) (4.11.4)\n",
      "Collecting gym-notices>=0.0.4\n",
      "  Using cached gym_notices-0.0.8-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from gym==0.25.2->gym-minigrid==0.0.5) (2.2.0)\n",
      "Requirement already satisfied: PyQt5-sip<13,>=12.11 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from pyqt5>=5.10.1->gym-minigrid==0.0.5) (12.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/saturncloud/envs/saturn/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gym==0.25.2->gym-minigrid==0.0.5) (3.8.1)\n",
      "Installing collected packages: gym-notices, gym, gym-minigrid\n",
      "  Running setup.py develop for gym-minigrid\n",
      "Successfully installed gym-0.25.2 gym-minigrid-0.0.5 gym-notices-0.0.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:50.372049Z",
     "iopub.status.busy": "2022-12-12T00:44:50.371626Z",
     "iopub.status.idle": "2022-12-12T00:44:50.466515Z",
     "shell.execute_reply": "2022-12-12T00:44:50.465750Z",
     "shell.execute_reply.started": "2022-12-12T00:44:50.372020Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/examples/examples/tensorflow/CSCI_7000_FinalProject/gym-minigrid/gym_minigrid/roomgrid.py:300: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if front_cell is None or front_cell.type is 'wall':\n",
      "/opt/saturncloud/envs/saturn/lib/python3.9/site-packages/gym/utils/seeding.py:63: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
      "  deprecation(\n",
      "/home/jovyan/examples/examples/tensorflow/CSCI_7000_FinalProject/gym-minigrid/gym_minigrid/minigrid.py:663: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask = np.zeros(shape=(grid.width, grid.height), dtype=np.bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'image': array([[[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [4, 4, 2],\n",
       "          [2, 5, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [5, 4, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0],\n",
       "          [1, 0, 0]],\n",
       "  \n",
       "         [[2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0],\n",
       "          [2, 5, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]],\n",
       "  \n",
       "         [[0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0],\n",
       "          [0, 0, 0]]], dtype=uint8),\n",
       "  'direction': 1,\n",
       "  'mission': 'Pickup the key, Open the door, avoid the lava and get to the goal'},\n",
       " -1,\n",
       " False,\n",
       " (1, 1))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "#env = CrossingEnv(size=6)\n",
    "env = MixedEnv(size=8)\n",
    "env.step_m(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:57.838979Z",
     "iopub.status.busy": "2022-12-12T00:44:57.838592Z",
     "iopub.status.idle": "2022-12-12T00:44:58.909986Z",
     "shell.execute_reply": "2022-12-12T00:44:58.909379Z",
     "shell.execute_reply.started": "2022-12-12T00:44:57.838954Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import gym_minigrid\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import LSTM,Bidirectional,Dense,Input,Embedding,TimeDistributed\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:44:58.912260Z",
     "iopub.status.busy": "2022-12-12T00:44:58.911342Z",
     "iopub.status.idle": "2022-12-12T00:44:58.915783Z",
     "shell.execute_reply": "2022-12-12T00:44:58.914988Z",
     "shell.execute_reply.started": "2022-12-12T00:44:58.912224Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_actions=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:00.948553Z",
     "iopub.status.busy": "2022-12-12T00:45:00.948155Z",
     "iopub.status.idle": "2022-12-12T00:45:00.952492Z",
     "shell.execute_reply": "2022-12-12T00:45:00.951743Z",
     "shell.execute_reply.started": "2022-12-12T00:45:00.948528Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plotProgress(reward_plot):\n",
    "    plt.plot(reward_plot)\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Avg. reward')\n",
    "    plt.title('Avg Reward Per Step V/S Episodes.')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:05.450799Z",
     "iopub.status.busy": "2022-12-12T00:45:05.450402Z",
     "iopub.status.idle": "2022-12-12T00:45:05.454130Z",
     "shell.execute_reply": "2022-12-12T00:45:05.453479Z",
     "shell.execute_reply.started": "2022-12-12T00:45:05.450770Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frame_offset = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:27.174676Z",
     "iopub.status.busy": "2022-12-12T00:45:27.174280Z",
     "iopub.status.idle": "2022-12-12T00:45:27.179842Z",
     "shell.execute_reply": "2022-12-12T00:45:27.179152Z",
     "shell.execute_reply.started": "2022-12-12T00:45:27.174651Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create():\n",
    "    global num_actions\n",
    "    input=Input(shape=((frame_offset+1),147)) # (3,3,7,7) ~ the mini-grid by default returns (3,7,7) image.\n",
    "    model=LSTM(units=256,return_sequences=False)(input)\n",
    "    x1 = Dense(units=256, activation='relu')(model)\n",
    "    x1 = Dense(units=128, activation='relu')(x1)\n",
    "    x1 = Dense(units=64, activation='relu')(x1)\n",
    "    x1 = Dense(units=32, activation='relu')(x1)\n",
    "    x1 = Dense(units=16, activation='relu')(x1)\n",
    "    output=Dense(units=num_actions,activation='linear')(x1)\n",
    "    model=Model(input,output)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:45:28.935891Z",
     "iopub.status.busy": "2022-12-12T00:45:28.935509Z",
     "iopub.status.idle": "2022-12-12T00:45:30.140921Z",
     "shell.execute_reply": "2022-12-12T00:45:30.140151Z",
     "shell.execute_reply.started": "2022-12-12T00:45:28.935865Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-12 00:45:28.948671: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-12 00:45:28.949139: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:45:28.949839: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:45:28.950420: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:45:29.607369: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:45:29.608138: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:45:29.609025: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-12-12 00:45:29.609796: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13795 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:1e.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.functional.Functional at 0x7fc42fba3520>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:34.451001Z",
     "iopub.status.busy": "2022-12-12T00:55:34.450612Z",
     "iopub.status.idle": "2022-12-12T00:55:35.056769Z",
     "shell.execute_reply": "2022-12-12T00:55:35.056025Z",
     "shell.execute_reply.started": "2022-12-12T00:55:34.450972Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "global num_actions\n",
    "# Configuration paramaters for the whole setup\n",
    "seed = 42\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "epsilon = 0.8  # Epsilon greedy parameter\n",
    "epsilon_min = 0.2  # Minimum epsilon greedy parameter\n",
    "epsilon_max = 0.8  # Maximum epsilon greedy parameter\n",
    "epsilon_interval = (\n",
    "    epsilon_max - epsilon_min\n",
    ")  # Rate at which to reduce chance of random action being taken\n",
    "batch_size = 4096  # Size of batch taken from replay buffer\n",
    "max_steps_per_episode = 5000 #beast 1000\n",
    "from gym_minigrid.envs.doorkey import *\n",
    "from gym_minigrid.envs.crossing import *\n",
    "from gym_minigrid.envs.mixed import *\n",
    "# env = DoorKeyEnv(size=8)\n",
    "#env = CrossingEnv(size=6)\n",
    "env = MixedEnv(siz=8)\n",
    "#env = gym.make('MiniGrid-DoorKey-6x6-v0')\n",
    "env.seed(seed)\n",
    "\n",
    "mixed_model=create()\n",
    "mixed_model_target=create()\n",
    "loss_function = keras.losses.MeanSquaredError()\n",
    "optimizer=keras.optimizers.RMSprop()\n",
    "\n",
    "# Experience replay buffers\n",
    "action_history = []\n",
    "state_history = []\n",
    "state_next_history = []\n",
    "rewards_history = []\n",
    "episode_reward_history=[]\n",
    "done_history = []\n",
    "reward_plot = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "frame_count = 0\n",
    "# Number of frames to take random action and observe output\n",
    "epsilon_random_frames = 100000 #beast 10000\n",
    "# Number of frames for exploration\n",
    "epsilon_greedy_frames = 500000.0\n",
    "# Maximum replay length\n",
    "# Note: The Deepmind paper suggests 1000000 however this causes memory issues\n",
    "max_memory_length = 200000\n",
    "# Train the model after 4 actions\n",
    "update_after_actions = 100\n",
    "# How often to update the target network\n",
    "update_target_network = 10000\n",
    "# Using huber loss for stability\n",
    "# We are taking 3 frames in our LSTM\n",
    "#frame_offset=2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:35.346253Z",
     "iopub.status.busy": "2022-12-12T00:55:35.345801Z",
     "iopub.status.idle": "2022-12-12T00:55:35.350172Z",
     "shell.execute_reply": "2022-12-12T00:55:35.349463Z",
     "shell.execute_reply.started": "2022-12-12T00:55:35.346211Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "termination_steps=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:35.911253Z",
     "iopub.status.busy": "2022-12-12T00:55:35.910885Z",
     "iopub.status.idle": "2022-12-12T00:55:35.914724Z",
     "shell.execute_reply": "2022-12-12T00:55:35.913950Z",
     "shell.execute_reply.started": "2022-12-12T00:55:35.911227Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "success_ratio = []\n",
    "epsilon_history = [epsilon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:37.258043Z",
     "iopub.status.busy": "2022-12-12T00:55:37.257654Z",
     "iopub.status.idle": "2022-12-12T00:55:37.263169Z",
     "shell.execute_reply": "2022-12-12T00:55:37.262508Z",
     "shell.execute_reply.started": "2022-12-12T00:55:37.258017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 42\n",
    "np.random.seed(seed=42)\n",
    "def best_action(action_probs):\n",
    "  action = np.argmax(action_probs)\n",
    "  action_probs = np.abs(action_probs - action_probs[action])\n",
    "  possible_actions = []\n",
    "  for i in range(5):\n",
    "    if (action_probs[i] < 0.2):\n",
    "      possible_actions.append(i)\n",
    "  # print(action_probs, possible_actions)\n",
    "  length = len(possible_actions)\n",
    "  action_probs = [1 / length for i in possible_actions]\n",
    "  return np.random.choice(possible_actions, p=action_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-12T00:55:39.013389Z",
     "iopub.status.busy": "2022-12-12T00:55:39.013007Z",
     "iopub.status.idle": "2022-12-12T00:55:39.034027Z",
     "shell.execute_reply": "2022-12-12T00:55:39.033251Z",
     "shell.execute_reply.started": "2022-12-12T00:55:39.013362Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 4, 147)]          0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 256)               413696    \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 5)                 85        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 523,333\n",
      "Trainable params: 523,333\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "mixed_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T07:48:26.009450Z",
     "iopub.status.busy": "2022-12-11T07:48:26.009065Z",
     "iopub.status.idle": "2022-12-11T07:48:30.134633Z",
     "shell.execute_reply": "2022-12-11T07:48:30.133776Z",
     "shell.execute_reply.started": "2022-12-11T07:48:26.009425Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "mixed_agent_targets = []\n",
    "mixed_agent_targets.append(keras.models.load_model(\"DoorKey_Experts1\"))\n",
    "mixed_agent_targets.append(keras.models.load_model(\"Lava_Experts2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T07:48:39.442438Z",
     "iopub.status.busy": "2022-12-11T07:48:39.442034Z",
     "iopub.status.idle": "2022-12-11T07:54:21.268923Z",
     "shell.execute_reply": "2022-12-11T07:54:21.268172Z",
     "shell.execute_reply.started": "2022-12-11T07:48:39.442413Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -7.411764705882353 17 -126\n",
      "0 -11.5 10 -115\n",
      "0 -8.785714285714286 14 -123\n",
      "0 -8.785714285714286 14 -123\n",
      "0 -8.266666666666667 15 -124\n",
      "0 -8.785714285714286 14 -123\n",
      "0 -10.909090909090908 11 -120\n",
      "0 -11.9 10 -119\n",
      "0 -10.083333333333334 12 -121\n",
      "0 -7.411764705882353 17 -126\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n",
      "1 -1.0002000400080016 4999 -5000\n"
     ]
    }
   ],
   "source": [
    "max_steps = 5000\n",
    "episodes = 100\n",
    "experts_rewards = []\n",
    "for e in range(len(mixed_agent_targets)):\n",
    "    avg_reward = 0\n",
    "    for i in range(episodes):\n",
    "        state = np.array(env.reset_m())\n",
    "        episode_reward = 0\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history = [temp_state['image']]\n",
    "        for i in range(frame_offset):\n",
    "            action = np.random.choice(5)\n",
    "            state, reward, done, _ = env.step_m(action)\n",
    "            state_numpy = np.array(state)\n",
    "            temp_state = dict(state_numpy.item(0))\n",
    "            state_history.append(temp_state['image'])\n",
    "        #print(np.array(state_history[-8:]).shape)\n",
    "        for j in range(max_steps):\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):])\n",
    "            #print(state_numpy.shape)\n",
    "            state_numpy = state_numpy.reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = mixed_agent_targets[e](state_numpy, training=False)\n",
    "            action = np.argmax(action_probs[0])\n",
    "            state, reward, done, _ = env.step_m(action)\n",
    "            state_numpy = np.array(state)\n",
    "            temp_state = dict(state_numpy.item(0))\n",
    "            del state_history[:1]\n",
    "            state_history.append(temp_state['image'])\n",
    "            # state_history.append(state)\n",
    "            #print(done,action)\n",
    "            episode_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        episode_avg_reward = episode_reward/j\n",
    "        avg_reward += episode_avg_reward\n",
    "        print(e,episode_avg_reward,j,episode_reward)\n",
    "    experts_rewards.append(avg_reward/episodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-11T07:54:21.270924Z",
     "iopub.status.busy": "2022-12-11T07:54:21.270370Z",
     "iopub.status.idle": "2022-12-11T07:54:21.278674Z",
     "shell.execute_reply": "2022-12-11T07:54:21.277736Z",
     "shell.execute_reply.started": "2022-12-11T07:54:21.270897Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-9.383976317799846, -1.0002000400080016]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax(arr):\n",
    "  for i in range(len(arr)):\n",
    "    print(np.exp(arr[i])/np.sum(np.exp(arr)))\n",
    "weights = softmax(experts_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateNovice(novice,exp,rewardArr):\n",
    "  m=novice.get_weights()\n",
    "  wt=[]\n",
    "  for xx in range(len(m)):\n",
    "    temp=0\n",
    "    for _ in range(N_EXPERTS):\n",
    "      temp+=np.array(exp[_].get_weights()[xx])*rewardArr[_]\n",
    "    wt.append(temp)\n",
    "  novice.set_weights(wt)\n",
    "\n",
    "novice=create()\n",
    "updateNovice(novice,mixed_agent_targets,rewardArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novice_target = create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noOfEpisodes=100 #beast 100000\n",
    "#while noOfEpisodes:  # Run until solved\n",
    "for _ in tqdm(range(noOfEpisodes)):\n",
    "    #noOfEpisodes-=1\n",
    "    state = np.array(env.reset_m())\n",
    "    episode_reward = 0\n",
    "    success = 0\n",
    "\n",
    "    for timestep in range(1, max_steps_per_episode):\n",
    "        # env.render(); Adding this line would show the attempts\n",
    "        # of the agent in a pop up window.\n",
    "        frame_count += 1\n",
    "\n",
    "        # Use epsilon-greedy for exploration\n",
    "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
    "            # Take random action\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            # Predict action Q-values\n",
    "            # From environment state\n",
    "            #print(len(state_history))\n",
    "            state_numpy = np.array(state_history[-(frame_offset+1):]).reshape((frame_offset+1),147)\n",
    "            state_numpy = np.array([state_numpy])\n",
    "            action_probs = novice(state_numpy, training=False)\n",
    "            # Take best action\n",
    "            #action = np.argmax(action_probs[0])\n",
    "            action = best_action(action_probs[0])\n",
    "            #print(action_probs[0])\n",
    "            \n",
    "\n",
    "        # Decay probability of taking random action\n",
    "        epsilon -= epsilon_interval / epsilon_greedy_frames\n",
    "        epsilon = max(epsilon, epsilon_min)\n",
    "        #epsilon_history.append(epsilon)\n",
    "        \n",
    "\n",
    "        # Apply the sampled action in our environment\n",
    "        state_next, reward, done, _ = env.step_m(action)\n",
    "        state_next = np.array(state_next)\n",
    "\n",
    "        episode_reward += reward\n",
    "        if(done==True):\n",
    "            success = 1\n",
    "\n",
    "        # Save actions and states in replay buffer\n",
    "        action_history.append(action)\n",
    "        temp_state=dict(state.item(0)) # state is a 0-d numpy array.\n",
    "        state_history.append(temp_state['image'])\n",
    "        temp_state=dict(state_next.item(0)) # state is a 0-d numpy array.\n",
    "        state_next_history.append(temp_state['image'])\n",
    "        done_history.append(done)\n",
    "        rewards_history.append(reward)\n",
    "        state = state_next\n",
    "\n",
    "        # Update every fourth frame and once batch size is over 32\n",
    "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
    "            # Get indices of samples for replay buffers\n",
    "            indices = np.random.choice(range(len(done_history)-frame_offset), size=batch_size)+frame_offset\n",
    "            i=indices[0]\n",
    "            # Using list comprehension to sample from replay buffer\n",
    "            state_sample = np.array([ np.array(state_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            state_next_sample = np.array([ np.array(state_next_history[i-frame_offset:i+1]).reshape((frame_offset+1),147) for i in indices])\n",
    "            rewards_sample = [rewards_history[i] for i in indices]\n",
    "            action_sample = [action_history[i] for i in indices]\n",
    "            # Build the updated Q-values for the sampled future states\n",
    "            # Use the target model for stability\n",
    "            future_rewards = novice_target.predict(state_next_sample,verbose=False)\n",
    "            # Q value = reward + discount factor * expected future reward\n",
    "            updated_q_values = rewards_sample + gamma * np.max(\\\n",
    "                future_rewards, axis=1)\n",
    "            updated_q_values = updated_q_values.astype('float32')\n",
    "            # Create a mask so we only calculate loss on the updated Q-values\n",
    "            masks = tf.one_hot(action_sample, num_actions)\n",
    "            with tf.GradientTape() as tape:    \n",
    "                q_values = novice(state_sample)\n",
    "                # Apply the masks to the Q-values to get the Q-value for action taken\n",
    "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
    "                loss = loss_function(updated_q_values,q_action)\n",
    "            # Backpropagation\n",
    "            grads = tape.gradient(loss, novice.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, novice.trainable_variables))\n",
    "            \n",
    "\n",
    "        if frame_count % update_target_network == 0:\n",
    "            # update the the target network with new weights\n",
    "            novice_target.set_weights(novice.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}, frame count {} with epsilon {}\"\n",
    "            print(template.format(running_reward, episode_count, frame_count, epsilon))\n",
    "        if frame_count > update_target_network*10:\n",
    "            update_target_network = update_target_network*10\n",
    "\n",
    "        # Limit the state and reward history\n",
    "        if len(rewards_history) > max_memory_length:\n",
    "            del rewards_history[:1]\n",
    "            del state_history[:1]\n",
    "            del state_next_history[:1]\n",
    "            del action_history[:1]\n",
    "            del done_history[:1]\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    success_ratio.append(success)\n",
    "    # Update running reward to check condition for solving\n",
    "    epsilon_history.append(epsilon)\n",
    "    episode_reward_history.append(episode_reward)\n",
    "    running_reward = np.mean(episode_reward_history)\n",
    "    reward_plot.append(episode_reward/timestep)\n",
    "    episode_count += 1\n",
    "    termination_steps.append(timestep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novice.save(\"MixedTransferLearning\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
